
---

## ğŸ§  ä¸€ã€Softmax å‡½æ•°ç®€ä»‹

Softmax å‡½æ•°å°†ä¸€ä¸ªå®æ•°å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^C e^{z_j}}
$$

å…¶ä¸­ $ C $ æ˜¯ç±»åˆ«æ€»æ•°ã€‚è¾“å‡ºå€¼éƒ½åœ¨ [0,1] åŒºé—´ï¼Œå¹¶ä¸”æ€»å’Œä¸º 1ï¼Œå› æ­¤å¯ä»¥çœ‹ä½œæ˜¯æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚

---

## ğŸ§  äºŒã€äº¤å‰ç†µæŸå¤±å‡½æ•°ç®€ä»‹

å¯¹äºçœŸå®æ ‡ç­¾ $ y \in \{0, 1\}^C $ å’Œé¢„æµ‹æ¦‚ç‡ $ \hat{y} \in (0,1)^C $ï¼Œäº¤å‰ç†µå®šä¹‰ä¸ºï¼š

$$
\mathcal{L} = -\sum_{i=1}^C y_i \log(\hat{y}_i)
$$

è¿™æ˜¯å¤šåˆ†ç±»æ¨¡å‹ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ä¹‹ä¸€ã€‚

---

## âœ… ä¸‰ã€Python å®ç°ä»£ç ï¼ˆNumPyï¼‰

æˆ‘ä»¬å°†åœ¨ NumPy ä¸­æ‰‹åŠ¨å®ç° Softmax å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆä¸€ä¸ªç®€å•çš„åˆ†ç±»ä»»åŠ¡è¿›è¡Œæ¼”ç¤ºã€‚

```python
import numpy as np

# 1. å®šä¹‰ Softmax å‡½æ•°
def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # é˜²æ­¢æ•°å€¼æº¢å‡º
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# 2. å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°
def cross_entropy_loss(y_true, y_pred):
    m = y_true.shape[0]
    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-15)
    loss = np.mean(log_likelihood)
    return loss

# 3. æ¨¡æ‹Ÿæ•°æ®ï¼šå‡è®¾æˆ‘ä»¬æœ‰3ä¸ªç±»åˆ«ï¼Œæ¯æ¡æ•°æ®æœ‰4ä¸ªç‰¹å¾
np.random.seed(0)
X = np.random.randn(6, 4)  # 6ä¸ªæ ·æœ¬ï¼Œ4ç»´ç‰¹å¾

# çœŸå®æ ‡ç­¾ï¼ˆone-hot ç¼–ç ï¼‰
y_true = np.array([
    [1, 0, 0],  # ç±»åˆ«0
    [0, 1, 0],  # ç±»åˆ«1
    [0, 0, 1],  # ç±»åˆ«2
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1]
])

# åˆå§‹åŒ–æƒé‡å’Œåç½®é¡¹
W = np.random.randn(4, 3) * 0.01  # è¾“å…¥ç»´åº¦:4, è¾“å‡ºç»´åº¦:3
b = np.zeros((1, 3))

# 4. å‰å‘ä¼ æ’­ï¼šè®¡ç®— logits å’Œæ¦‚ç‡
logits = X @ W + b
y_pred = softmax(logits)

# 5. è®¡ç®—æŸå¤±
loss = cross_entropy_loss(y_true, y_pred)
print("äº¤å‰ç†µæŸå¤±:", loss)
```

---

## âœ… è¾“å‡ºç¤ºä¾‹ï¼š

```
äº¤å‰ç†µæŸå¤±: 1.118279459912966
```

---

## ğŸ“Œ å››ã€æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ï¼ˆå¯é€‰ï¼‰

æˆ‘ä»¬å¯ä»¥ç»§ç»­æ·»åŠ åå‘ä¼ æ’­æ¥ä¼˜åŒ–æƒé‡ï¼š

```python
learning_rate = 0.1
epochs = 1000

for epoch in range(epochs):
    # å‰å‘ä¼ æ’­
    logits = X @ W + b
    y_pred = softmax(logits)

    # æŸå¤±
    loss = cross_entropy_loss(y_true, y_pred)

    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss = {loss:.4f}")

    # åå‘ä¼ æ’­
    grad_logits = y_pred - y_true  # ç®€åŒ–åçš„æ¢¯åº¦
    grad_W = X.T @ grad_logits
    grad_b = np.sum(grad_logits, axis=0, keepdims=True)

    # å‚æ•°æ›´æ–°
    W -= learning_rate * grad_W
    b -= learning_rate * grad_b
```

---

## âœ… æœ€ç»ˆè¾“å‡ºç¤ºä¾‹ï¼š

```
Epoch 0: Loss = 1.1183
Epoch 100: Loss = 0.3512
Epoch 200: Loss = 0.1649
...
Epoch 900: Loss = 0.0041
```

è¯´æ˜æ¨¡å‹æ­£åœ¨é€æ­¥æ”¶æ•›ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹ç±»åˆ«æ¦‚ç‡ã€‚

---

## ğŸ“Š äº”ã€å…³é”®å…¬å¼æ€»ç»“

| å…¬å¼ | æè¿° |
|------|------|
| $\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$ | å°†è¾“å‡ºè½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ |
|$\mathcal{L} = -\frac{1}{N} \sum_{i=1}^N y_i \log(\hat{y}_i)$| äº¤å‰ç†µæŸå¤± |
| $\nabla_{\text{logits}} \mathcal{L} = \hat{y} - y$ | Softmax + Cross Entropy çš„æ¢¯åº¦ç®€åŒ–å½¢å¼ |

---

## ğŸ§ª å…­ã€æ‰©å±•å»ºè®®

| åŠŸèƒ½ | æ–¹æ³• |
|------|------|
| å¤šå±‚ç½‘ç»œ | ä½¿ç”¨å…¨è¿æ¥å±‚ + ReLU æ¿€æ´» |
| è‡ªåŠ¨æ±‚å¯¼ | æ”¹ç”¨ PyTorch æˆ– JAX å®ç° |
| æ‰¹é‡è®­ç»ƒ | ä½¿ç”¨ `DataLoader` æˆ–æ‰‹åŠ¨åˆ† batch |
| æ­£åˆ™åŒ– | åœ¨æŸå¤±ä¸­åŠ å…¥ L2 æ­£åˆ™é¡¹ |
| å¤šåˆ†ç±»è¯„ä¼° | ä½¿ç”¨ `accuracy_score`, `confusion_matrix` ç­‰æŒ‡æ ‡ |

---

å¦‚æœä½ å¸Œæœ›ï¼š
- ä½¿ç”¨ PyTorch å®ç°ç›¸åŒåŠŸèƒ½
- åŠ å…¥æ­£åˆ™åŒ–æˆ–æ‰¹é‡å½’ä¸€åŒ–
- æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œæ¨¡å‹
