使用 **NumPy** 可以非常方便地实现 **线性回归（Linear Regression）**，其中最常用的方法是利用最小二乘法（Least Squares），通过 `np.linalg.lstsq` 函数求解模型的系数。

---

## 📌 一、什么是线性回归？

线性回归是一种建模方法，用于建立一个自变量 $ x $（或多个自变量）与因变量 $ y $ 之间的线性关系：

$$
y = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n
$$

- $ w_i $：模型参数（权重）
- 目标：找到一组最佳参数 $ \mathbf{w} $，使得预测值尽可能接近真实值

---

## ✅ 二、用 NumPy 实现线性回归步骤

### 示例：一元线性回归（单变量）

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. 构造数据
np.random.seed(0)
X = np.random.rand(100, 1) * 10  # 输入特征（100个样本，1个特征）
y = 2.5 * X.squeeze() + 1.2 + np.random.randn(100) * 2  # 真实关系 + 噪声

# 2. 添加偏置项（即 w0*x0，x0=1）
X_b = np.hstack([np.ones((X.shape[0], 1)), X])  # 添加一列全为1的列（截距项）

# 3. 使用最小二乘法求解系数
# 求解 Ax = b 的最小二乘解，返回 (系数, 残差, ...)
coefficients, residuals, rank, singular_values = np.linalg.lstsq(X_b, y, rcond=None)

# 输出模型参数
print("模型参数（截距和斜率）:", coefficients)

# 4. 进行预测
y_pred = X_b @ coefficients

# 5. 可视化结果
plt.scatter(X, y, color='blue', label='原始数据')
plt.plot(X, y_pred, color='red', label='拟合直线')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('NumPy 实现线性回归')
plt.grid(True)
plt.show()
```

---

## 🔍 输出示例：

```
模型参数（截距和斜率）: [1.19876282 2.52490246]
```

说明：
- 截距（bias）约为 1.2
- 斜率（weight）约为 2.5
- 与我们构造的数据一致！

---

## 🧠 三、函数说明

| 方法 | 含义 |
|------|------|
| `np.hstack()` | 将两个数组水平拼接，用于添加偏置项 |
| `np.linalg.lstsq()` | 求解最小二乘问题，适用于欠定、适定、超定系统 |
| `@` 或 `np.dot()` | 矩阵乘法 |

---

## ✅ 四、多元线性回归（多变量）示例

你可以轻松扩展到多个输入变量：

```python
# 生成两个特征的输入数据
X_multi = np.random.rand(100, 2) * 10
y_multi = 1.5 * X_multi[:, 0] + 2.0 * X_multi[:, 1] + 3.0 + np.random.randn(100)

# 添加偏置项
X_multi_b = np.hstack([np.ones((X_multi.shape[0], 1)), X_multi])

# 求解系数
coeffs_multi, _, _, _ = np.linalg.lstsq(X_multi_b, y_multi, rcond=None)

print("多元线性回归模型参数:", coeffs_multi)
```

---

## 📌 五、总结

| 步骤 | 内容 |
|------|------|
| 构造数据 | 创建带噪声的真实数据 |
| 添加偏置项 | 用于学习截距项（bias） |
| 最小二乘法 | 使用 `np.linalg.lstsq` 求解线性模型系数 |
| 预测 | 用训练好的系数进行预测 |
| 可视化 | 绘图展示拟合效果 |

---

## 🧩 六、拓展建议

- 使用 `scikit-learn` 中的 `LinearRegression` 进行对比
- 计算 R² 分数评估模型性能
- 对比正规方程（Normal Equation）与梯度下降（Gradient Descent）

---