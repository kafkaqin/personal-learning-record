ä½¿ç”¨ **NumPy** å¯ä»¥éå¸¸æ–¹ä¾¿åœ°å®ç° **çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰**ï¼Œå…¶ä¸­æœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯åˆ©ç”¨æœ€å°äºŒä¹˜æ³•ï¼ˆLeast Squaresï¼‰ï¼Œé€šè¿‡ `np.linalg.lstsq` å‡½æ•°æ±‚è§£æ¨¡å‹çš„ç³»æ•°ã€‚

---

## ğŸ“Œ ä¸€ã€ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ

çº¿æ€§å›å½’æ˜¯ä¸€ç§å»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºå»ºç«‹ä¸€ä¸ªè‡ªå˜é‡ $ x $ï¼ˆæˆ–å¤šä¸ªè‡ªå˜é‡ï¼‰ä¸å› å˜é‡ $ y $ ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼š

$$
y = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n
$$

- $ w_i $ï¼šæ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰
- ç›®æ ‡ï¼šæ‰¾åˆ°ä¸€ç»„æœ€ä½³å‚æ•° $ \mathbf{w} $ï¼Œä½¿å¾—é¢„æµ‹å€¼å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼

---

## âœ… äºŒã€ç”¨ NumPy å®ç°çº¿æ€§å›å½’æ­¥éª¤

### ç¤ºä¾‹ï¼šä¸€å…ƒçº¿æ€§å›å½’ï¼ˆå•å˜é‡ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. æ„é€ æ•°æ®
np.random.seed(0)
X = np.random.rand(100, 1) * 10  # è¾“å…¥ç‰¹å¾ï¼ˆ100ä¸ªæ ·æœ¬ï¼Œ1ä¸ªç‰¹å¾ï¼‰
y = 2.5 * X.squeeze() + 1.2 + np.random.randn(100) * 2  # çœŸå®å…³ç³» + å™ªå£°

# 2. æ·»åŠ åç½®é¡¹ï¼ˆå³ w0*x0ï¼Œx0=1ï¼‰
X_b = np.hstack([np.ones((X.shape[0], 1)), X])  # æ·»åŠ ä¸€åˆ—å…¨ä¸º1çš„åˆ—ï¼ˆæˆªè·é¡¹ï¼‰

# 3. ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ±‚è§£ç³»æ•°
# æ±‚è§£ Ax = b çš„æœ€å°äºŒä¹˜è§£ï¼Œè¿”å› (ç³»æ•°, æ®‹å·®, ...)
coefficients, residuals, rank, singular_values = np.linalg.lstsq(X_b, y, rcond=None)

# è¾“å‡ºæ¨¡å‹å‚æ•°
print("æ¨¡å‹å‚æ•°ï¼ˆæˆªè·å’Œæ–œç‡ï¼‰:", coefficients)

# 4. è¿›è¡Œé¢„æµ‹
y_pred = X_b @ coefficients

# 5. å¯è§†åŒ–ç»“æœ
plt.scatter(X, y, color='blue', label='åŸå§‹æ•°æ®')
plt.plot(X, y_pred, color='red', label='æ‹Ÿåˆç›´çº¿')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('NumPy å®ç°çº¿æ€§å›å½’')
plt.grid(True)
plt.show()
```

---

## ğŸ” è¾“å‡ºç¤ºä¾‹ï¼š

```
æ¨¡å‹å‚æ•°ï¼ˆæˆªè·å’Œæ–œç‡ï¼‰: [1.19876282 2.52490246]
```

è¯´æ˜ï¼š
- æˆªè·ï¼ˆbiasï¼‰çº¦ä¸º 1.2
- æ–œç‡ï¼ˆweightï¼‰çº¦ä¸º 2.5
- ä¸æˆ‘ä»¬æ„é€ çš„æ•°æ®ä¸€è‡´ï¼

---

## ğŸ§  ä¸‰ã€å‡½æ•°è¯´æ˜

| æ–¹æ³• | å«ä¹‰ |
|------|------|
| `np.hstack()` | å°†ä¸¤ä¸ªæ•°ç»„æ°´å¹³æ‹¼æ¥ï¼Œç”¨äºæ·»åŠ åç½®é¡¹ |
| `np.linalg.lstsq()` | æ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜ï¼Œé€‚ç”¨äºæ¬ å®šã€é€‚å®šã€è¶…å®šç³»ç»Ÿ |
| `@` æˆ– `np.dot()` | çŸ©é˜µä¹˜æ³• |

---

## âœ… å››ã€å¤šå…ƒçº¿æ€§å›å½’ï¼ˆå¤šå˜é‡ï¼‰ç¤ºä¾‹

ä½ å¯ä»¥è½»æ¾æ‰©å±•åˆ°å¤šä¸ªè¾“å…¥å˜é‡ï¼š

```python
# ç”Ÿæˆä¸¤ä¸ªç‰¹å¾çš„è¾“å…¥æ•°æ®
X_multi = np.random.rand(100, 2) * 10
y_multi = 1.5 * X_multi[:, 0] + 2.0 * X_multi[:, 1] + 3.0 + np.random.randn(100)

# æ·»åŠ åç½®é¡¹
X_multi_b = np.hstack([np.ones((X_multi.shape[0], 1)), X_multi])

# æ±‚è§£ç³»æ•°
coeffs_multi, _, _, _ = np.linalg.lstsq(X_multi_b, y_multi, rcond=None)

print("å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹å‚æ•°:", coeffs_multi)
```

---

## ğŸ“Œ äº”ã€æ€»ç»“

| æ­¥éª¤ | å†…å®¹ |
|------|------|
| æ„é€ æ•°æ® | åˆ›å»ºå¸¦å™ªå£°çš„çœŸå®æ•°æ® |
| æ·»åŠ åç½®é¡¹ | ç”¨äºå­¦ä¹ æˆªè·é¡¹ï¼ˆbiasï¼‰ |
| æœ€å°äºŒä¹˜æ³• | ä½¿ç”¨ `np.linalg.lstsq` æ±‚è§£çº¿æ€§æ¨¡å‹ç³»æ•° |
| é¢„æµ‹ | ç”¨è®­ç»ƒå¥½çš„ç³»æ•°è¿›è¡Œé¢„æµ‹ |
| å¯è§†åŒ– | ç»˜å›¾å±•ç¤ºæ‹Ÿåˆæ•ˆæœ |

---

## ğŸ§© å…­ã€æ‹“å±•å»ºè®®

- ä½¿ç”¨ `scikit-learn` ä¸­çš„ `LinearRegression` è¿›è¡Œå¯¹æ¯”
- è®¡ç®— RÂ² åˆ†æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½
- å¯¹æ¯”æ­£è§„æ–¹ç¨‹ï¼ˆNormal Equationï¼‰ä¸æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰

---