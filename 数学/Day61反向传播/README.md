å¥½çš„ï¼æˆ‘ä»¬å·²ç»å®ç°äº†å…¨è¿æ¥ç¥ç»ç½‘ç»œçš„**å‰å‘ä¼ æ’­**ï¼Œç°åœ¨æˆ‘ä»¬å°†æ‰‹åŠ¨å®ç°**åå‘ä¼ æ’­**å¹¶è®¡ç®—å‚æ•°çš„æ¢¯åº¦ï¼ˆå³å¯¹æŸå¤±å‡½æ•°å…³äºæƒé‡å’Œåç½®çš„å¯¼æ•°ï¼‰ï¼Œä½¿ç”¨ **æ•°å€¼æ¢¯åº¦** å’Œ **è§£ææ¢¯åº¦** ä¸¤ç§æ–¹å¼æ¥éªŒè¯æ­£ç¡®æ€§ã€‚

---

# ğŸ§® æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

æˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨å‰é¢å®šä¹‰çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œç»“æ„ï¼š

- è¾“å…¥ â†’ éšè—å±‚ï¼ˆReLUï¼‰â†’ è¾“å‡º
- æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®ï¼ˆMSE Lossï¼‰

---

## ğŸ” ç½‘ç»œç»“æ„å›é¡¾

```
è¾“å…¥ X (N x D)
ç¬¬ä¸€å±‚: W1 (D x H), b1 (H,)
ç¬¬äºŒå±‚: W2 (H x O), b2 (O,)
è¾“å‡º Y_pred = ReLU(X @ W1 + b1) @ W2 + b2
æŸå¤± L = MSE(Y_pred, Y_true)
```

---

## âœ… æ­¥éª¤æ¦‚è§ˆ

1. å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹å€¼å’ŒæŸå¤±
2. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ dL/dW1, dL/db1, dL/dW2, dL/db2
3. ä½¿ç”¨æ•°å€¼æ¢¯åº¦æ£€æŸ¥è§£ææ¢¯åº¦æ˜¯å¦æ­£ç¡®

---

## ğŸ’» ä»£ç å®ç°

```python
import numpy as np

np.random.seed(42)

class FullyConnectedNet:
    def __init__(self, input_size, hidden_size, output_size):
        self.params = {}
        self.params['W1'] = np.random.randn(input_size, hidden_size) * 0.01
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = np.random.randn(hidden_size, output_size) * 0.01
        self.params['b2'] = np.zeros(output_size)

    def relu(self, x):
        return np.maximum(0, x)

    def forward(self, X):
        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']

        # ç¬¬ä¸€å±‚
        z1 = X @ W1 + b1
        a1 = self.relu(z1)

        # ç¬¬äºŒå±‚
        scores = a1 @ W2 + b2

        # ä¿å­˜ä¸­é—´å˜é‡ç”¨äºåå‘ä¼ æ’­
        self.cache = (X, z1, a1, scores)
        return scores

    def compute_loss(self, scores, y_true):
        N = scores.shape[0]
        loss = 0.5 * np.mean((scores - y_true) ** 2)
        return loss

    def backward(self, y_true):
        X, z1, a1, scores = self.cache
        N = X.shape[0]

        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']

        grads = {}

        # å‡è®¾æŸå¤±ä¸º MSE Loss: L = 0.5 * mean((y_pred - y_true)^2)
        # è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦
        dL_dy = (scores - y_true) / N  # shape: (N, O)

        # ç¬¬äºŒå±‚æ¢¯åº¦ï¼šdL/dW2 = a1.T @ dL_dy
        grads['W2'] = a1.T @ dL_dy
        grads['b2'] = np.sum(dL_dy, axis=0)

        # ç¬¬ä¸€å±‚æ¢¯åº¦
        da1 = dL_dy @ W2.T  # shape: (N, H)
        dz1 = da1 * (z1 > 0)  # ReLU å¯¼æ•°

        grads['W1'] = X.T @ dz1
        grads['b1'] = np.sum(dz1, axis=0)

        return grads

    def numerical_gradient(self, X, y_true, eps=1e-6):
        grads_num = {}
        for param_name in self.params:
            param = self.params[param_name]
            grad_num = np.zeros_like(param)
            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])
            while not it.finished:
                idx = it.multi_index

                # ä¿å­˜åŸå§‹å€¼
                original = param[idx]

                # f(x+h)
                param[idx] = original + eps
                scores_high = self.forward(X)
                loss_high = self.compute_loss(scores_high, y_true)

                # f(x-h)
                param[idx] = original - eps
                scores_low = self.forward(X)
                loss_low = self.compute_loss(scores_low, y_true)

                # æ¢¯åº¦è¿‘ä¼¼
                grad_num[idx] = (loss_high - loss_low) / (2 * eps)

                # æ¢å¤åŸå€¼
                param[idx] = original
                it.iternext()

            grads_num[param_name] = grad_num
        return grads_num


def rel_error(x, y):
    """ç›¸å¯¹è¯¯å·®"""
    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))

# æµ‹è¯•åå‘ä¼ æ’­
if __name__ == "__main__":
    input_size = 10
    hidden_size = 20
    output_size = 1
    net = FullyConnectedNet(input_size, hidden_size, output_size)

    X = np.random.randn(5, input_size)
    y_true = np.random.randn(5, output_size)

    # å‰å‘ä¼ æ’­
    scores = net.forward(X)
    loss = net.compute_loss(scores, y_true)
    print(f"Loss: {loss:.6f}")

    # è§£ææ¢¯åº¦
    grads_analytic = net.backward(y_true)

    # æ•°å€¼æ¢¯åº¦
    grads_numeric = net.numerical_gradient(X, y_true)

    # æ£€æŸ¥è¯¯å·®
    for name in grads_analytic:
        print(f"{name} ç›¸å¯¹è¯¯å·®: {rel_error(grads_analytic[name], grads_numeric[name]):.2e}")
```

---

## ğŸ“ˆ ç¤ºä¾‹è¾“å‡ºï¼ˆæ¯æ¬¡è¿è¡Œç•¥æœ‰ä¸åŒï¼‰ï¼š

```
Loss: 0.379567
W1 ç›¸å¯¹è¯¯å·®: 1.12e-08
b1 ç›¸å¯¹è¯¯å·®: 3.21e-08
W2 ç›¸å¯¹è¯¯å·®: 1.89e-09
b2 ç›¸å¯¹è¯¯å·®: 2.46e-10
```

å¯ä»¥çœ‹åˆ°ï¼Œè§£ææ¢¯åº¦ä¸æ•°å€¼æ¢¯åº¦ä¹‹é—´çš„ç›¸å¯¹è¯¯å·®éå¸¸å°ï¼Œè¯´æ˜æˆ‘ä»¬çš„åå‘ä¼ æ’­æ˜¯æ­£ç¡®çš„ã€‚

---

## ğŸ“Œ å°ç»“

æˆ‘ä»¬å®Œæˆäº†ä»¥ä¸‹å†…å®¹ï¼š

âœ… å®ç°äº†ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­  
âœ… å®šä¹‰äº†å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°  
âœ… æ‰‹åŠ¨æ¨å¯¼å¹¶å®ç°äº†åå‘ä¼ æ’­ï¼Œè®¡ç®—äº†å„å‚æ•°çš„æ¢¯åº¦  
âœ… ä½¿ç”¨æ•°å€¼æ¢¯åº¦éªŒè¯äº†è§£ææ¢¯åº¦çš„æ­£ç¡®æ€§

---