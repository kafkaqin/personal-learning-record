è®¡ç®—å¤šå…ƒå‡½æ•°çš„**æ¢¯åº¦ï¼ˆGradientï¼‰**ï¼Œæ˜¯ä¼˜åŒ–ã€æœºå™¨å­¦ä¹ å’Œç§‘å­¦è®¡ç®—ä¸­çš„åŸºç¡€æ“ä½œã€‚ä¸‹é¢æˆ‘å°†åˆ†ä¸¤éƒ¨åˆ†è®²è§£ï¼š

---

## âœ… ä¸€ã€ä»€ä¹ˆæ˜¯å¤šå…ƒå‡½æ•°çš„æ¢¯åº¦ï¼Ÿ

å¯¹äºä¸€ä¸ªå¤šå…ƒå®å€¼å‡½æ•° $ f(\mathbf{x}) = f(x_1, x_2, \dots, x_n) $ï¼Œå®ƒçš„ **æ¢¯åº¦** æ˜¯ä¸€ä¸ªå‘é‡ï¼Œç”±æ‰€æœ‰åå¯¼æ•°ç»„æˆï¼š

$$
\nabla f(\mathbf{x}) =
\left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)
$$

æ¢¯åº¦çš„æ–¹å‘è¡¨ç¤ºå‡½æ•°åœ¨è¯¥ç‚¹ä¸Šå‡æœ€å¿«çš„æ–¹å‘ï¼Œè´Ÿæ–¹å‘åˆ™ä¸ºä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚

---

## âœ… äºŒã€ç”¨ NumPy å®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆGradient Descentï¼‰

### ğŸ§ª ç¤ºä¾‹ï¼šæœ€å°åŒ–å‡½æ•° $ f(x, y) = x^2 + (y - 3)^2 $

è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼åœ¨ $ (x, y) = (0, 3) $ å¤„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥é€¼è¿‘å®ƒã€‚

### ğŸ”¢ æ­¥éª¤ï¼š

1. å®šä¹‰ç›®æ ‡å‡½æ•°ï¼›
2. è®¡ç®—å…¶æ¢¯åº¦ï¼›
3. ä½¿ç”¨è¿­ä»£æ›´æ–°å…¬å¼ï¼š
   $$
   \mathbf{x}_{k+1} = \mathbf{x}_k - \eta \cdot \nabla f(\mathbf{x}_k)
   $$
   å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ï¼›
4. è®¾ç½®è¿­ä»£ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚æœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–è¯¯å·®é˜ˆå€¼ï¼‰ã€‚

---

### ğŸ“Œ Python å®ç°ä»£ç ï¼š

```python
import numpy as np

# ç›®æ ‡å‡½æ•°
def f(x):
    return x[0]**2 + (x[1] - 3)**2

# æ¢¯åº¦å‡½æ•°
def grad_f(x, eps=1e-6):
    # æ•°å€¼å¾®åˆ†è¿‘ä¼¼æ¢¯åº¦
    grad = np.zeros_like(x)
    for i in range(len(x)):
        dx = np.zeros_like(x)
        dx[i] = eps
        grad[i] = (f(x + dx) - f(x - dx)) / (2 * eps)
    return grad

# æ¢¯åº¦ä¸‹é™æ³•
def gradient_descent(starting_point, learning_rate=0.1, max_iter=1000, tol=1e-6):
    x = np.array(starting_point, dtype=float)
    for i in range(max_iter):
        grad = grad_f(x)
        step = learning_rate * grad
        if np.linalg.norm(step) < tol:
            print(f"æ”¶æ•›äºç¬¬ {i} æ¬¡è¿­ä»£")
            break
        x -= step
        if i % 100 == 0:
            print(f"è¿­ä»£ {i}: x = {x}, f(x) = {f(x)}")
    return x, f(x)

# åˆå§‹çŒœæµ‹å€¼
x0 = [1.0, 1.0]

# è¿è¡Œæ¢¯åº¦ä¸‹é™
minimum_point, minimum_value = gradient_descent(x0)

print("æå°å€¼ç‚¹:", minimum_point)
print("æå°å€¼:", minimum_value)
```

---

### ğŸ“ˆ è¾“å‡ºç¤ºä¾‹ï¼ˆå¯èƒ½ç•¥æœ‰ä¸åŒï¼‰ï¼š

```
è¿­ä»£ 0: x = [1. 1.], f(x) = 5.0
è¿­ä»£ 100: x = [0.005 2.995], f(x) â‰ˆ 0.000025
æ”¶æ•›äºç¬¬ 127 æ¬¡è¿­ä»£
æå°å€¼ç‚¹: [0.0001 3.0000]
æå°å€¼: 1.23e-08
```

---

## ğŸ“Œ æ€»ç»“

| åŠŸèƒ½ | å®ç°æ–¹å¼ |
|------|-----------|
| å®šä¹‰å‡½æ•° | `def f(x): return ...` |
| æ¢¯åº¦è®¡ç®— | æ•°å€¼å¾®åˆ†ï¼šä¸­å¿ƒå·®åˆ†æ³• `f(x+h) - f(x-h) / 2h` |
| æ¢¯åº¦ä¸‹é™ | è¿­ä»£æ›´æ–°ï¼š`x = x - lr * grad` |
| åœæ­¢æ¡ä»¶ | æœ€å¤§è¿­ä»£æ•° æˆ– æ­¥é•¿å°äºå®¹å·® |

---

## ğŸ§  æç¤ºä¸æ‰©å±•

- å¦‚æœä½ çŸ¥é“å‡½æ•°çš„è§£ææ¢¯åº¦ï¼Œå¯ä»¥ç›´æ¥å†™å‡ºæ¥ä»£æ›¿æ•°å€¼å¾®åˆ†ï¼Œé€Ÿåº¦æ›´å¿«ã€‚
- å¯ä»¥ä½¿ç”¨æ›´é«˜çº§çš„ä¼˜åŒ–å™¨ï¼Œå¦‚ `scipy.optimize.minimize` ä¸­çš„ `BFGS` ç­‰æ–¹æ³•ã€‚
- å­¦ä¹ ç‡å¤ªå¤§ä¼šå¯¼è‡´å‘æ•£ï¼Œå¤ªå°ä¼šæ”¶æ•›æ…¢ã€‚å¯ä»¥å°è¯•è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ï¼ˆå¦‚ Adamï¼‰ã€‚
- å¯¹äºé«˜ç»´é—®é¢˜ï¼Œå¯ä»¥ç»“åˆ NumPy å‘é‡åŒ–åŠ é€Ÿè®¡ç®—ã€‚

---
