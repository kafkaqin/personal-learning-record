å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ **NumPy** å®ç°çš„ **ç®€å•å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFully Connected Neural Networkï¼‰çš„å‰å‘ä¼ æ’­è¿‡ç¨‹**ï¼Œé€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ã€‚

æˆ‘ä»¬å°†å®ç°ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼ˆè¾“å…¥å±‚ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚ï¼‰ï¼š

---

## ğŸ§  ä¸€ã€æ¨¡å‹ç»“æ„

- è¾“å…¥å±‚ï¼šå¤§å° `input_size`ï¼ˆä¾‹å¦‚ç‰¹å¾ç»´åº¦ï¼‰
- éšè—å±‚ï¼šå¤§å° `hidden_size`ï¼Œä½¿ç”¨ **ReLU æ¿€æ´»å‡½æ•°**
- è¾“å‡ºå±‚ï¼šå¤§å° `output_size`ï¼Œä½¿ç”¨ **Softmax æ¿€æ´»å‡½æ•°**
- æŸå¤±å‡½æ•°ï¼š**äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy Lossï¼‰**

---

## âœ… äºŒã€Python ç¤ºä¾‹ä»£ç ï¼ˆNumPyï¼‰

```python
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
np.random.seed(42)

# æ¨¡æ‹Ÿæ•°æ®
num_samples = 100
input_size = 4
hidden_size = 10
output_size = 3

# éšæœºç”Ÿæˆä¸€äº›è¾“å…¥æ•°æ®å’Œ one-hot ç¼–ç çš„æ ‡ç­¾
X = np.random.randn(num_samples, input_size)
y_true = np.eye(output_size)[np.random.choice(output_size, num_samples)]

# åˆå§‹åŒ–å‚æ•°
W1 = np.random.randn(input_size, hidden_size) * 0.01
b1 = np.zeros((1, hidden_size))
W2 = np.random.randn(hidden_size, output_size) * 0.01
b2 = np.zeros((1, output_size))

# å®šä¹‰æ¿€æ´»å‡½æ•°
def relu(z):
    return np.maximum(0, z)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # æ•°å€¼ç¨³å®šæ€§å¤„ç†
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# å‰å‘ä¼ æ’­
def forward(X, W1, b1, W2, b2):
    # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢ + ReLU æ¿€æ´»
    z1 = X @ W1 + b1
    a1 = relu(z1)

    # ç¬¬äºŒå±‚çº¿æ€§å˜æ¢ + Softmax è¾“å‡º
    z2 = a1 @ W2 + b2
    y_pred = softmax(z2)

    return y_pred, a1, z1

# æ‰§è¡Œå‰å‘ä¼ æ’­
y_pred, a1, z1 = forward(X, W1, b1, W2, b2)

# æŸ¥çœ‹è¾“å‡ºå½¢çŠ¶
print("è¾“å…¥ X å½¢çŠ¶:", X.shape)
print("é¢„æµ‹æ¦‚ç‡ y_pred å½¢çŠ¶:", y_pred.shape)
print("éšè—å±‚æ¿€æ´»å€¼ a1 å½¢çŠ¶:", a1.shape)
```

---

## âœ… è¾“å‡ºç¤ºä¾‹ï¼š

```
è¾“å…¥ X å½¢çŠ¶: (100, 4)
é¢„æµ‹æ¦‚ç‡ y_pred å½¢çŠ¶: (100, 3)
éšè—å±‚æ¿€æ´»å€¼ a1 å½¢çŠ¶: (100, 10)
```

è¯´æ˜ï¼š
- æ¯ä¸ªæ ·æœ¬æœ‰ 4 ä¸ªç‰¹å¾ï¼Œç»è¿‡éšè—å±‚åå˜ä¸º 10 ç»´ã€‚
- æœ€ç»ˆè¾“å‡ºæ˜¯ 3 ç±»çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆSoftmax è¾“å‡ºï¼‰ã€‚

---

## ğŸ“Œ ä¸‰ã€å…³é”®å…¬å¼æ€»ç»“

| å±‚çº§ | å…¬å¼ |
|------|------|
| éšè—å±‚ï¼ˆReLUï¼‰ | $ a^{(1)} = \text{ReLU}(X W^{(1)} + b^{(1)}) $ |
| è¾“å‡ºå±‚ï¼ˆSoftmaxï¼‰ | $ \hat{y} = \text{Softmax}(a^{(1)} W^{(2)} + b^{(2)}) $ |

---

## ğŸ§ª å››ã€åç»­æ‰©å±•å»ºè®®

ä½ å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­æ·»åŠ ä»¥ä¸‹åŠŸèƒ½ï¼š

| åŠŸèƒ½ | å®ç°æ–¹å¼ |
|------|----------|
| åå‘ä¼ æ’­ | è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•° |
| æŸå¤±è®¡ç®— | ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•° |
| å¤šè½®è®­ç»ƒ | æ·»åŠ  for å¾ªç¯è¿›è¡Œè¿­ä»£ä¼˜åŒ– |
| ä½¿ç”¨è‡ªåŠ¨æ±‚å¯¼ | æ”¹ä¸º PyTorch æˆ– JAX ç‰ˆæœ¬ |
| åŠ å…¥æ­£åˆ™åŒ– | åœ¨æŸå¤±ä¸­åŠ å…¥ L2 æ­£åˆ™é¡¹ |

---

## ğŸ“Š äº”ã€å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆç®€è¦ç¤ºæ„ï¼‰

å¦‚æœä½ æƒ³ç»§ç»­å®ç°åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ï¼Œè¿™é‡Œæ˜¯ç®€åŒ–çš„æ­¥éª¤ï¼š

```python
learning_rate = 1e-3

for epoch in range(100):
    # å‰å‘ä¼ æ’­
    y_pred, a1, z1 = forward(X, W1, b1, W2, b2)

    # è®¡ç®—æŸå¤±ï¼ˆç•¥ï¼‰

    # åå‘ä¼ æ’­
    dy = y_pred - y_true
    dW2 = a1.T @ dy
    db2 = np.sum(dy, axis=0, keepdims=True)

    da1 = dy @ W2.T
    dz1 = da1 * (z1 > 0)  # ReLU å¯¼æ•°
    dW1 = X.T @ dz1
    db1 = np.sum(dz1, axis=0, keepdims=True)

    # å‚æ•°æ›´æ–°
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
```

---

