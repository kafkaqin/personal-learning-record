å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥å®ç°ä¸€ä¸ª**ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**ï¼Œæ¯”å¦‚**åŠ æ€§æ³¨æ„åŠ›ï¼ˆAdditive Attentionï¼‰**ï¼Œä¹Ÿç§°ä¸º **Bahdanau Attention**ï¼Œå®ƒæœ€åˆç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼ˆSequence-to-Sequenceï¼‰ï¼Œèƒ½å¸®åŠ©æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—ä¸­æ›´ç›¸å…³çš„éƒ¨åˆ†ã€‚

---

## ğŸ§  æ³¨æ„åŠ›æœºåˆ¶ç®€ä»‹

æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†å½“å‰è¾“å‡ºæ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­ä¸åŒçš„ä½ç½®ã€‚å¸¸è§çš„æ³¨æ„åŠ›ç±»å‹åŒ…æ‹¬ï¼š

| ç±»å‹ | ç‰¹ç‚¹ |
|------|------|
| åŠ æ€§æ³¨æ„åŠ›ï¼ˆAdditive / Bahdanauï¼‰ | ä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„éšè—å±‚æ¥è®¡ç®—æ³¨æ„åŠ›å¾—åˆ† |
| ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Productï¼‰ | ç®€å•è®¡ç®— query å’Œ key çš„ç‚¹ç§¯ |
| ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Productï¼‰ | Transformer ä¸­ä½¿ç”¨ï¼Œç‚¹ç§¯åé™¤ä»¥ $\sqrt{d_k}$ |

---

## âœ… åŠ æ€§æ³¨æ„åŠ›ï¼ˆAdditive Attentionï¼‰å…¬å¼

ç»™å®šï¼š

- **query**ï¼šå½“å‰è§£ç å™¨çŠ¶æ€ï¼ˆå¦‚ï¼š$ h_t $ï¼‰
- **keys**ï¼šæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€ï¼ˆå¦‚ï¼š$ h_1, h_2, ..., h_T $ï¼‰

è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼š

$$
e_i = v^T \tanh(W h_i + b)
$$

ç„¶åä½¿ç”¨ softmax å½’ä¸€åŒ–ï¼š

$$
\alpha_i = \frac{e^{e_i}}{\sum_j e^{e_j}}
$$

æœ€ååŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡ï¼š

$$
c = \sum_i \alpha_i h_i
$$

---

## ğŸ’» PyTorch å®ç°åŠ æ€§æ³¨æ„åŠ›å±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AdditiveAttention(nn.Module):
    def __init__(self, hidden_dim):
        super(AdditiveAttention, self).__init__()
        self.W = nn.Linear(hidden_dim, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)
        
    def forward(self, query, keys):
        """
        Args:
            query: [batch_size, hidden_dim]ï¼Œå½“å‰è§£ç å™¨çŠ¶æ€
            keys:  [batch_size, seq_len, hidden_dim]ï¼Œæ‰€æœ‰ç¼–ç å™¨çŠ¶æ€

        Returns:
            context: [batch_size, hidden_dim]ï¼ŒåŠ æƒä¸Šä¸‹æ–‡å‘é‡
            weights: [batch_size, seq_len]ï¼Œæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
        """
        # Step 1: è®¡ç®— W * h_i
        energy = self.W(keys)  # shape: [batch_size, seq_len, hidden_dim]

        # Step 2: åŠ ä¸Š queryï¼ˆå¹¿æ’­æœºåˆ¶ï¼‰
        # query.unsqueeze(1): [batch_size, 1, hidden_dim]
        # energy: [batch_size, seq_len, hidden_dim]
        energy = torch.tanh(energy + query.unsqueeze(1))

        # Step 3: è®¡ç®— v^T * tanh(...)
        energy = self.v(energy).squeeze(-1)  # shape: [batch_size, seq_len]

        # Step 4: softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        weights = F.softmax(energy, dim=1)  # shape: [batch_size, seq_len]

        # Step 5: åŠ æƒæ±‚å’Œ
        context = torch.bmm(weights.unsqueeze(1), keys).squeeze(1)  # shape: [batch_size, hidden_dim]

        return context, weights
```

---

## ğŸ§ª ç¤ºä¾‹ç”¨æ³•

```python
# å‡è®¾ batch_size=2, seq_len=5, hidden_dim=10
batch_size = 2
seq_len = 5
hidden_dim = 10

# éšæœºç”Ÿæˆ query å’Œ keys
query = torch.randn(batch_size, hidden_dim)  # å½“å‰è§£ç å™¨çŠ¶æ€
keys = torch.randn(batch_size, seq_len, hidden_dim)  # æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€

# å®ä¾‹åŒ–æ³¨æ„åŠ›å±‚
attention = AdditiveAttention(hidden_dim)

# å‰å‘ä¼ æ’­
context, weights = attention(query, keys)

print("ä¸Šä¸‹æ–‡å‘é‡ shape:", context.shape)  # [2, 10]
print("æ³¨æ„åŠ›æƒé‡ shape:", weights.shape)  # [2, 5]
print("æ³¨æ„åŠ›æƒé‡ç¤ºä¾‹:\n", weights)
```

---

## ğŸ“ˆ æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰

```python
import matplotlib.pyplot as plt

# ç”»å‡ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡
plt.bar(range(seq_len), weights[0].detach().numpy())
plt.xlabel("è¾“å…¥ä½ç½®")
plt.ylabel("æ³¨æ„åŠ›æƒé‡")
plt.title("Additive Attention Weights")
plt.show()
```

---

## âœ… æ€»ç»“å¯¹æ¯”è¡¨

| æ–¹æ³• | å…¬å¼ | é€‚ç”¨åœºæ™¯ | æ˜¯å¦å¯å¾® |
|------|------|----------|-----------|
| åŠ æ€§æ³¨æ„åŠ› | $ v^T \tanh(W h_i + b) $ | åºåˆ—å»ºæ¨¡ã€RNN-based æ¨¡å‹ | âœ… æ˜¯ |
| ç‚¹ç§¯æ³¨æ„åŠ› | $ q \cdot k_i $ | å¿«é€Ÿè®¡ç®—ï¼Œç»´åº¦ä¸€è‡´æ—¶ | âœ… æ˜¯ |
| ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› | $ \frac{q \cdot k_i}{\sqrt{d_k}} $ | Transformer ç­‰æ·±å±‚æ¨¡å‹ | âœ… æ˜¯ |

---

## ğŸ§© è¿›ä¸€æ­¥æ‰©å±•å»ºè®®

ä½ å¯ä»¥ç»§ç»­ï¼š

- æŠŠæ³¨æ„åŠ›æœºåˆ¶é›†æˆåˆ° Seq2Seq æ¨¡å‹ä¸­ï¼ˆå¦‚ Encoder-Decoderï¼‰
- å®ç° **ç‚¹ç§¯æ³¨æ„åŠ›** æˆ– **å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-head Attentionï¼‰**
- ä½¿ç”¨ `nn.MultiheadAttention` æ¨¡å—
- åœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
- ä½¿ç”¨ `torch.nn.utils.rnn.pack_padded_sequence` å¤„ç†å˜é•¿åºåˆ—

---