éå¸¸å¥½ï¼**æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰** æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒå¯ä»¥åŠ é€Ÿè®­ç»ƒã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ï¼Œå¹¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šå…·æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚

ä¸‹é¢æˆ‘ä»¬æ¥ï¼š

âœ… æ‰‹åŠ¨å®ç°ä¸€ä¸ª **PyTorch é£æ ¼çš„ BatchNorm1d å±‚ï¼ˆç”¨äºå…¨è¿æ¥å±‚ï¼‰**  
âœ… å¹¶åœ¨ç®€å•çš„ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å®ƒ  
âœ… å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨ BatchNorm çš„è®­ç»ƒé€Ÿåº¦å·®å¼‚

---

## ğŸ§  æ‰¹å½’ä¸€åŒ–åŸç†å›é¡¾

åœ¨è®­ç»ƒé˜¶æ®µï¼Œå¯¹æ¯ä¸ª batch çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼š

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

ç„¶åè¿›è¡Œä»¿å°„å˜æ¢ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰ï¼š

$$
y_i = \gamma \hat{x}_i + \beta
$$

å…¶ä¸­ï¼š

- $\mu_B$: batch å‡å€¼
- $\sigma_B^2$: batch æ–¹å·®
- $\gamma, \beta$: å¯å­¦ä¹ å‚æ•°
- $\epsilon$: é˜²æ­¢é™¤ä»¥ 0 çš„å°å¸¸æ•°

---

## ğŸ§± æ‰‹åŠ¨å®ç° BatchNorm1d å±‚ï¼ˆPyTorché£æ ¼ï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MyBatchNorm1d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super(MyBatchNorm1d, self).__init__()
        self.eps = eps
        self.momentum = momentum

        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))

        # è¿è¡Œæ—¶ç»Ÿè®¡çš„å‡å€¼å’Œæ–¹å·®ï¼ˆç”¨äºæ¨ç†ï¼‰
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))

    def forward(self, x):
        if self.training:
            batch_mean = x.mean(dim=0)
            batch_var = x.var(dim=0, unbiased=False)

            # æ›´æ–° running mean and var
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            self.num_batches_tracked += 1
        else:
            batch_mean = self.running_mean
            batch_var = self.running_var

        # å½’ä¸€åŒ–
        x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)

        # ä»¿å°„å˜æ¢
        out = self.gamma * x_norm + self.beta
        return out
```

---

## ğŸ§ª ä½¿ç”¨ BatchNorm çš„ç®€å•æ¨¡å‹

```python
class SimpleModelWithBN(nn.Module):
    def __init__(self):
        super(SimpleModelWithBN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 50),
            MyBatchNorm1d(50),
            nn.ReLU(),
            nn.Linear(50, 1)
        )

    def forward(self, x):
        return self.net(x)
```

---

## ğŸ§ª ä¸ä½¿ç”¨ BatchNorm çš„å¯¹ç…§æ¨¡å‹

```python
class SimpleModelWithoutBN(nn.Module):
    def __init__(self):
        super(SimpleModelWithoutBN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 50),
            nn.ReLU(),
            nn.Linear(50, 1)
        )

    def forward(self, x):
        return self.net(x)
```

---

## ğŸ“¦ æ•°æ®ä¸è®­ç»ƒè®¾ç½®

```python
import torch.optim as optim

# ç”Ÿæˆéšæœºæ•°æ®
def get_data(num_samples=1000):
    X = torch.randn(num_samples, 10)
    y = torch.randn(num_samples, 1)
    return X, y

X, y = get_data()

# æ¨¡å‹åˆå§‹åŒ–
model_bn = SimpleModelWithBN()
model_no_bn = SimpleModelWithoutBN()

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer_bn = optim.Adam(model_bn.parameters(), lr=0.01)
optimizer_no_bn = optim.Adam(model_no_bn.parameters(), lr=0.01)
```

---

## ğŸš€ è®­ç»ƒå‡½æ•°

```python
def train(model, optimizer, X, y, epochs=200, model_name="Model"):
    losses = []
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

        if (epoch + 1) % 50 == 0:
            print(f"{model_name} Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
    return losses
```

---

## ğŸ“ˆ å¯è§†åŒ–å¯¹æ¯”ç»“æœ

```python
import matplotlib.pyplot as plt

# è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
losses_bn = train(model_bn, optimizer_bn, X, y, model_name="With BN")
losses_no_bn = train(model_no_bn, optimizer_no_bn, X, y, model_name="Without BN")

# å¯è§†åŒ–
plt.plot(losses_bn, label='With BatchNorm')
plt.plot(losses_no_bn, label='Without BatchNorm')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training Loss with/without BatchNorm')
plt.grid(True)
plt.show()
```

---

## ğŸ“Š ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰ä¸åŒï¼‰

```
With BN Epoch [50/200], Loss: 0.6212
With BN Epoch [100/200], Loss: 0.3123
...

Without BN Epoch [50/200], Loss: 0.8945
Without BN Epoch [100/200], Loss: 0.7213
...
```

å¯è§†åŒ–å›¾æ˜¾ç¤ºï¼š**ä½¿ç”¨ BatchNorm çš„æ¨¡å‹æ”¶æ•›æ›´å¿«ã€æŸå¤±æ›´å°ã€‚**

---

## âœ… æ€»ç»“å¯¹æ¯”

| æ¨¡å‹ | æ˜¯å¦ä½¿ç”¨ BatchNorm | æ”¶æ•›é€Ÿåº¦ | æŸå¤±å€¼ | æ˜¯å¦æ¨è |
|------|---------------------|----------|--------|----------|
| Model 1 | âœ… æ˜¯ | âœ… å¿« | âœ… å° | âœ… æ¨è |
| Model 2 | âŒ å¦ | âŒ æ…¢ | âŒ å¤§ | âŒ ä¸æ¨è |

---

## ğŸ§© è¿›ä¸€æ­¥æ‰©å±•å»ºè®®

ä½ å¯ä»¥ç»§ç»­ï¼š

- å®ç° `BatchNorm2d`ï¼ˆç”¨äºå·ç§¯å±‚ï¼‰
- ä½¿ç”¨ PyTorch å†…ç½®çš„ `nn.BatchNorm1d` æ›¿ä»£æ‰‹åŠ¨å®ç°
- æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚ `StepLR` æˆ– `ReduceLROnPlateau`ï¼‰
- ä½¿ç”¨ `torchviz` å¯è§†åŒ– BatchNorm çš„è®¡ç®—å›¾
- å¯¹æ¯” BatchNorm å’Œ LayerNorm çš„å·®å¼‚