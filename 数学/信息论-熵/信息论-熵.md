åœ¨ä¿¡æ¯è®ºä¸­ï¼Œ**ç†µï¼ˆEntropyï¼‰** å’Œ **äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰** æ˜¯è¡¡é‡ç¦»æ•£å˜é‡ä¸ç¡®å®šæ€§ä¸ç›¸å…³æ€§çš„ä¸¤ä¸ªé‡è¦æŒ‡æ ‡ã€‚

---

## âœ… ä¸€ã€åŸºæœ¬æ¦‚å¿µ

### ğŸ§  1. ç†µï¼ˆEntropyï¼‰

æè¿°ä¸€ä¸ªç¦»æ•£éšæœºå˜é‡ $ X $ çš„ä¸ç¡®å®šæ€§ç¨‹åº¦ï¼š

$$
H(X) = -\sum_{i} p(x_i) \log_2 p(x_i)
$$

å…¶ä¸­ $ p(x_i) $ æ˜¯å˜é‡å–å€¼ä¸º $ x_i $ çš„æ¦‚ç‡ã€‚

> æ³¨æ„ï¼šå½“ $ p(x_i) = 0 $ æ—¶ï¼Œ$ p(x_i) \log_2 p(x_i) = 0 $

---

### ğŸ§  2. è”åˆç†µï¼ˆJoint Entropyï¼‰

å¯¹äºä¸¤ä¸ªå˜é‡ $ X $ å’Œ $ Y $ï¼š

$$
H(X, Y) = -\sum_{i,j} p(x_i, y_j) \log_2 p(x_i, y_j)
$$

---

### ğŸ§  3. æ¡ä»¶ç†µï¼ˆConditional Entropyï¼‰

è¡¨ç¤ºåœ¨å·²çŸ¥ $ Y $ çš„æ¡ä»¶ä¸‹ï¼Œ$ X $ çš„ä¸ç¡®å®šæ€§ï¼š

$$
H(X|Y) = H(X,Y) - H(Y)
$$

---

### ğŸ§  4. äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰

è¡¡é‡ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„â€œå…±äº«ä¿¡æ¯â€æˆ–â€œç›¸å…³æ€§â€ï¼š

$$
I(X;Y) = H(X) + H(Y) - H(X,Y)
$$

ä¹Ÿå¯ä»¥ç†è§£ä¸ºçŸ¥é“ $ Y $ åå¯¹ $ X $ ä¸ç¡®å®šæ€§çš„å‡å°‘é‡ã€‚

---

## âœ… äºŒã€Python å®ç°ç¤ºä¾‹

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `numpy` å’Œ `collections` æ¥å®ç°è¿™äº›åº¦é‡ã€‚

```python
import numpy as np
from collections import Counter

def entropy(labels):
    """è®¡ç®—å•ä¸ªå˜é‡çš„ç†µ H(X)"""
    counts = Counter(labels)
    total = len(labels)
    probs = np.array([count / total for count in counts.values()])
    return -np.sum(probs * np.log2(probs))

def joint_entropy(X, Y):
    """è®¡ç®—è”åˆç†µ H(X,Y)"""
    pairs = list(zip(X, Y))
    counts = Counter(pairs)
    total = len(pairs)
    probs = np.array([count / total for count in counts.values()])
    return -np.sum(probs * np.log2(probs))

def mutual_information(X, Y):
    """è®¡ç®—äº’ä¿¡æ¯ I(X;Y)"""
    H_X = entropy(X)
    H_Y = entropy(Y)
    H_XY = joint_entropy(X, Y)
    return H_X + H_Y - H_XY
```

---

## âœ… ä¸‰ã€ä½¿ç”¨ç¤ºä¾‹

```python
# ç¤ºä¾‹æ•°æ®
X = ['a', 'a', 'b', 'b', 'a', 'b']
Y = [1,   1,   1,   2,   2,   2]

print("H(X):", entropy(X))              # è¾“å‡º: ~0.918
print("H(Y):", entropy(Y))              # è¾“å‡º: ~0.918
print("H(X,Y):", joint_entropy(X, Y))   # è¾“å‡º: ~1.47
print("I(X;Y):", mutual_information(X, Y))  # è¾“å‡º: ~0.36
```

---

## âœ… å››ã€è¾“å‡ºè§£é‡Š

- è‹¥ `I(X;Y)` æ¥è¿‘äº 0 â†’ å˜é‡å‡ ä¹æ— å…³
- è‹¥ `I(X;Y)` è¾ƒå¤§ â†’ è¡¨ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´æœ‰è¾ƒå¼ºçš„ç›¸å…³æ€§

---

## âœ… äº”ã€æ³¨æ„äº‹é¡¹

- è¾“å…¥åº”æ˜¯å¯æšä¸¾çš„ç¦»æ•£å˜é‡ï¼ˆå­—ç¬¦ä¸²ã€æ•´æ•°ç­‰ï¼‰
- å¦‚æœå˜é‡æ˜¯è¿ç»­çš„ï¼Œéœ€è¦å…ˆè¿›è¡Œç¦»æ•£åŒ–å¤„ç†
- å¯ä»¥ç”¨ `scikit-learn` ä¸­çš„ `mutual_info_classif` æˆ– `mutual_info_regression` æ¥æ›´é«˜æ•ˆåœ°è®¡ç®—äº’ä¿¡æ¯

---

## ğŸ“Œ å…­ã€åº”ç”¨åœºæ™¯ä¸¾ä¾‹

| åœºæ™¯ | åº”ç”¨ |
|------|------|
| ç‰¹å¾é€‰æ‹© | é€‰æ‹©ä¸ç›®æ ‡å˜é‡äº’ä¿¡æ¯é«˜çš„ç‰¹å¾ |
| NLP | è¯è¯­ä¸ç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æ |
| ç”Ÿç‰©ä¿¡æ¯å­¦ | åŸºå› è¡¨è¾¾ä¸ç–¾ç—…çŠ¶æ€çš„å…³ç³» |
| æ•°æ®æŒ–æ˜ | å‘ç°å˜é‡é—´çš„ä¾èµ–å…³ç³» |

---