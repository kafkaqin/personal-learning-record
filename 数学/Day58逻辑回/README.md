ä½¿ç”¨ **Sigmoid å‡½æ•°** å®ç°åˆ†ç±»æ˜¯é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰çš„æ ¸å¿ƒæ€æƒ³ã€‚Sigmoid å‡½æ•°å°†ä»»æ„å®æ•°æ˜ å°„åˆ° $[0, 1]$ åŒºé—´ï¼Œå› æ­¤éå¸¸é€‚åˆç”¨äºäºŒåˆ†ç±»é—®é¢˜çš„æ¦‚ç‡è¾“å‡ºã€‚

---

## ğŸ§  ä¸€ã€Sigmoid å‡½æ•°å®šä¹‰

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

- å½“ $ z \to +\infty $ï¼Œ$\sigma(z) \to 1$
- å½“ $ z \to -\infty $ï¼Œ$\sigma(z) \to 0$
- å½“ $ z = 0 $ï¼Œ$\sigma(z) = 0.5$

è¿™ä¸ªå‡½æ•°å¸¸ç”¨äºå°†çº¿æ€§è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡å€¼ï¼Œä»è€Œè¿›è¡ŒäºŒåˆ†ç±»ï¼š

$$
P(y=1|x) = \sigma(w^T x + b)
$$

---

## âœ… äºŒã€Python ç¤ºä¾‹ï¼šç”¨ NumPy å®ç° Sigmoid åˆ†ç±»å™¨

æˆ‘ä»¬ä»¥ä¸€ä¸ªç®€å•çš„äºŒç»´æ•°æ®é›†ä¸ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•æ‰‹åŠ¨å®ç° Sigmoid å‡½æ•°å’Œåˆ†ç±»é¢„æµ‹ã€‚

### 1. å®šä¹‰ Sigmoid å‡½æ•°

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

### 2. æ„é€ ç®€å•æ•°æ®ï¼ˆä¸¤ç±»ç‚¹ï¼‰

```python
# ç”Ÿæˆä¸¤ç±»æ•°æ®ç‚¹ï¼ˆç±»åˆ« 0 å’Œ 1ï¼‰
np.random.seed(42)
X_class0 = np.random.randn(50, 2) + [2, 2]
X_class1 = np.random.randn(50, 2) + [-2, -2]
X = np.vstack((X_class0, X_class1))

y = np.array([0]*50 + [1]*50).reshape(-1, 1)

# æ·»åŠ åç½®é¡¹ï¼ˆw0*x0 + w1*x1 + b => ä½¿ç”¨ w0*x0 + w1*x1 + w2*1ï¼‰
X_bias = np.hstack((X, np.ones((X.shape[0], 1))))
```

### 3. åˆå§‹åŒ–å‚æ•°å¹¶è®­ç»ƒï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

```python
# åˆå§‹åŒ–æƒé‡
weights = np.random.randn(3, 1)

learning_rate = 0.1
n_iterations = 1000

for i in range(n_iterations):
    # çº¿æ€§ç»„åˆ + sigmoid
    z = X_bias @ weights
    y_pred = sigmoid(z)

    # è®¡ç®—æŸå¤±ï¼ˆå¯é€‰ï¼‰
    if i % 200 == 0:
        loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))
        print(f"Iteration {i}: Loss = {loss:.4f}")

    # æ¢¯åº¦è®¡ç®—ï¼ˆäº¤å‰ç†µæŸå¤±çš„æ¢¯åº¦ï¼‰
    gradient = (y_pred - y) * X_bias
    gradient = np.mean(gradient, axis=0).reshape(-1, 1)

    # å‚æ•°æ›´æ–°
    weights -= learning_rate * gradient
```

### 4. é¢„æµ‹ä¸å¯è§†åŒ–

```python
# åˆ†ç±»å‡½æ•°
def predict(X, weights):
    z = X @ weights
    prob = sigmoid(z)
    return (prob >= 0.5).astype(int)

# æµ‹è¯•æ–°æ•°æ®ç‚¹
test_points = np.array([[3, 3], [-3, -3]])
test_points_bias = np.hstack((test_points, np.ones((test_points.shape[0], 1))))
predictions = predict(test_points_bias, weights)

print("æµ‹è¯•ç‚¹é¢„æµ‹ç»“æœ:")
for point, label in zip(test_points, predictions):
    print(f"ç‚¹ {point} è¢«é¢„æµ‹ä¸ºç±»åˆ« {label.item()}")
```

### 5. å¯è§†åŒ–å†³ç­–è¾¹ç•Œ

```python
x_vals = np.linspace(-5, 5, 100)
y_vals = -(weights[0] * x_vals + weights[2]) / weights[1]

plt.scatter(X_class0[:, 0], X_class0[:, 1], label="Class 0", color='blue')
plt.scatter(X_class1[:, 0], X_class1[:, 1], label="Class 1", color='red')
plt.plot(x_vals, y_vals, color='green', label='Decision Boundary')
plt.title('Sigmoid åˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œ')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.grid(True)
plt.show()
```

---

## âœ… è¾“å‡ºç¤ºä¾‹ï¼š

```
Iteration 0: Loss = 0.9678
Iteration 200: Loss = 0.2321
Iteration 400: Loss = 0.1421
Iteration 600: Loss = 0.0981
Iteration 800: Loss = 0.0735

æµ‹è¯•ç‚¹é¢„æµ‹ç»“æœ:
ç‚¹ [3. 3.] è¢«é¢„æµ‹ä¸ºç±»åˆ« 1
ç‚¹ [-3. -3.] è¢«é¢„æµ‹ä¸ºç±»åˆ« 0
```

---

## ğŸ“Œ ä¸‰ã€å…³é”®å…¬å¼æ€»ç»“

| å…¬å¼ | æè¿° |
|------|------|
| $\sigma(z) = \frac{1}{1 + e^{-z}}$ | Sigmoid å‡½æ•° |
| $y_{pred} = \sigma(w^T x + b)$ | æ¦‚ç‡é¢„æµ‹ |
| $\mathcal{L} = -\frac{1}{n} \sum y \log(y_{pred}) + (1-y)\log(1-y_{pred})$ | äº¤å‰ç†µæŸå¤± |
| $\nabla_w \mathcal{L} = \frac{1}{n} \sum (y_{pred} - y) x$ | æ¢¯åº¦ |

---

## ğŸ§ª å››ã€æ‰©å±•å»ºè®®

| åŠŸèƒ½ | æ–¹æ³• |
|------|------|
| å¤šåˆ†ç±» | ä½¿ç”¨ Softmax æ›¿ä»£ Sigmoid |
| æ­£åˆ™åŒ– | åœ¨æŸå¤±ä¸­åŠ å…¥ L2 æ­£åˆ™é¡¹ |
| å‘é‡åŒ–ä¼˜åŒ– | ä½¿ç”¨çŸ©é˜µè¿ç®—åŠ é€Ÿè®¡ç®— |
| ä½¿ç”¨ Scikit-Learn | `LogisticRegression` æ›´é«˜æ•ˆ |
| åŠ å…¥åå·®é¡¹ | å»ºè®®å§‹ç»ˆåŠ ä¸Šï¼Œå¦åˆ™æ¨¡å‹å¯èƒ½ä¸å‡†ç¡® |

---

å¦‚æœä½ å¸Œæœ›ï¼š
- æ‰©å±•ä¸ºå¤šå˜é‡é€»è¾‘å›å½’
- åŠ å…¥æ­£åˆ™åŒ–ï¼ˆL1/L2ï¼‰
- æ”¹ä¸ºæ‰¹é‡/éšæœºæ¢¯åº¦ä¸‹é™
- ä½¿ç”¨ PyTorch/JAX è‡ªåŠ¨æ±‚å¯¼ç‰ˆæœ¬

