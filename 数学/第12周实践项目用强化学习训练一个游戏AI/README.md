å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘å°†å¸¦ä½ ä½¿ç”¨ **å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰** è®­ç»ƒä¸€ä¸ªç© **Flappy Bird** çš„ AIã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `PyTorch` å®ç° **DQNï¼ˆDeep Q-Networkï¼‰** ç®—æ³•ï¼Œå¹¶ç»“åˆ `pygame` æ¥æ¨¡æ‹Ÿæ¸¸æˆç¯å¢ƒã€‚

è¿™ä¸ªé¡¹ç›®éå¸¸é€‚åˆç†è§£ RL çš„æ ¸å¿ƒæ¦‚å¿µï¼šçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€Q-learning å’Œç»éªŒå›æ”¾ã€‚

---

## ğŸ® é¡¹ç›®ç›®æ ‡

- è®­ç»ƒä¸€ä¸ª AI æ™ºèƒ½ä½“è‡ªåŠ¨ç© Flappy Birdã€‚
- ä½¿ç”¨ **DQN + Experience Replay + Target Network** æå‡ç¨³å®šæ€§ã€‚
- æœ€ç»ˆå®ç°æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­é£è¡Œå¹¶é¿å¼€ç®¡é“ã€‚

---

## ğŸ§° æ‰€éœ€å·¥å…·

```bash
pip install pygame torch numpy matplotlib
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
flappy_bird_rl/
â”œâ”€â”€ game/                   # Flappy Bird æ¸¸æˆé€»è¾‘ï¼ˆä½¿ç”¨ pygameï¼‰
â”œâ”€â”€ dqn_agent.py            # DQN æ™ºèƒ½ä½“
â”œâ”€â”€ train.py                # è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ model.pth               # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
â””â”€â”€ requirements.txt
```

---

## ğŸ§  ä¸€ã€DQN æ™ºèƒ½ä½“å®ç°ï¼ˆ`dqn_agent.py`ï¼‰

```python
# dqn_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        self.memory = deque(maxlen=50000)
        self.batch_size = 32

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # å½“å‰Qç½‘ç»œå’Œç›®æ ‡Qç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim).to(self.device)
        self.target_network = DQN(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        self.update_target_network()

    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim)
        state = torch.FloatTensor(state).to(self.device)
        q_values = self.q_network(state)
        return q_values.argmax().item()

    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor(np.vstack([e[0] for e in batch])).to(self.device)
        actions = torch.LongTensor(np.array([e[1] for e in batch])).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor(np.vstack([e[3] for e in batch])).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)

        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0]
        target_q_values = rewards + self.gamma * next_q_values * (~dones)
        target_q_values = target_q_values.unsqueeze(1)

        loss = F.mse_loss(current_q_values, target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

---

## ğŸ•¹ï¸ äºŒã€Flappy Bird ç¯å¢ƒç®€åŒ–ç‰ˆï¼ˆ`train.py` ä¸­é›†æˆï¼‰

æˆ‘ä»¬ä¸ä½¿ç”¨å®Œæ•´çš„ `pygame` ä»£ç ï¼Œè€Œæ˜¯æå–å…³é”®é€»è¾‘ã€‚ä½ å¯ä»¥ä»å¼€æºé¡¹ç›®è·å–æ¸¸æˆä»£ç ï¼Œè¿™é‡Œæˆ‘ä»¬åªå±•ç¤º**çŠ¶æ€æå–å’Œäº¤äº’é€»è¾‘**ã€‚

```python
# train.py
import pygame
import sys
import random
from dqn_agent import DQNAgent
import numpy as np

# åˆå§‹åŒ–æ¸¸æˆ
pygame.init()
screen = pygame.display.set_mode((400, 708))
clock = pygame.time.Clock()

# æ¸¸æˆå‚æ•°
gravity = 0.25
bird_movement = 0
score = 0

# åŠ è½½å›¾åƒï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€å‡†å¤‡èµ„æºï¼‰
# bird_surface = pygame.image.load('bird.png')
# pipe_surface = pygame.image.load('pipe.png')

# ç”Ÿæˆç®¡é“
def create_pipe():
    height = random.randint(150, 400)
    bottom_pipe = pygame.Rect(400, height, 60, 500)
    top_pipe = pygame.Rect(400, height - 700, 60, 500)
    return bottom_pipe, top_pipe

# ç§»åŠ¨ç®¡é“
def move_pipes(pipes):
    for pipe in pipes:
        pipe.centerx -= 3
    return [pipe for pipe in pipes if pipe.right > -50]

# æ£€æµ‹ç¢°æ’
def check_collision(pipes, bird_rect):
    for pipe in pipes:
        if bird_rect.colliderect(pipe):
            return True
    if bird_rect.top <= 0 or bird_rect.bottom >= 650:
        return True
    return False

# è·å–å½“å‰çŠ¶æ€ï¼ˆè¾“å…¥DQNï¼‰
def get_state(bird_y, bird_velocity, pipes):
    if not pipes:
        return np.array([bird_y / 700, bird_velocity / 10, 10, 10])  # é»˜è®¤è¿œå¤„ç®¡é“

    # æ‰¾æœ€è¿‘çš„å‰æ–¹ç®¡é“
    nearest_pipe = min([p for p in pipes if p.centerx > 50], key=lambda p: p.centerx, default=None)
    if nearest_pipe is None:
        dx = 400
        dy = 0
    else:
        dx = (nearest_pipe.centerx - 50) / 400  # å½’ä¸€åŒ–è·ç¦»
        dy = (nearest_pipe.top - bird_y) / 700

    return np.array([bird_y / 700, bird_velocity / 10, dx, dy])

# ä¸»è®­ç»ƒå¾ªç¯
def main():
    agent = DQNAgent(state_dim=4, action_dim=2)  # åŠ¨ä½œï¼šä¸è·³ / è·³
    episodes = 1000
    best_score = 0

    for episode in range(episodes):
        bird_rect = pygame.Rect(50, 350, 30, 30)
        pipes = []
        frame_count = 0
        score = 0
        done = False
        bird_movement = 0

        print(f"Episode {episode+1}/{episodes}, Epsilon: {agent.epsilon:.3f}")

        while not done:
            # å¥–åŠ±æœºåˆ¶
            reward = 0.1  # æ¯å­˜æ´»ä¸€å¸§ç»™å°å¥–åŠ±

            # äº‹ä»¶å¤„ç†
            action = 0
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    sys.exit()

            # DQN å†³ç­–
            state = get_state(bird_rect.centery, bird_movement, pipes)
            if np.random.rand() > 0.1:  # å°‘é‡éšæœºæ¢ç´¢
                action = agent.act(state)
            else:
                action = random.randrange(2)

            if action == 1:
                bird_movement = -6  # è·³è·ƒ

            # æ¸¸æˆæ›´æ–°
            bird_movement += gravity
            bird_rect.centery += bird_movement

            if frame_count % 150 == 0:
                b, t = create_pipe()
                pipes.extend([b, t])

            pipes = move_pipes(pipes)
            frame_count += 1

            # ç¢°æ’æ£€æµ‹
            if check_collision(pipes, bird_rect):
                done = True
                reward = -10  # æ­»äº¡æƒ©ç½š
            else:
                # æ¯é€šè¿‡ä¸€ä¸ªç®¡é“åŠ åˆ†
                for pipe in pipes:
                    if pipe.centerx == 50 and pipe.bottom > 600:
                        score += 1
                        reward = 10  # é€šè¿‡ç®¡é“å¥–åŠ±

            # ä¸‹ä¸€çŠ¶æ€
            next_state = get_state(bird_rect.centery, bird_movement, pipes)
            agent.remember(state, action, reward, next_state, done)

            # è®­ç»ƒ
            agent.replay()

            # ç»˜åˆ¶ï¼ˆå¯é€‰ï¼šè®­ç»ƒæ—¶å¯å…³é—­å›¾å½¢ç•Œé¢åŠ é€Ÿï¼‰
            screen.fill((135, 206, 250))
            pygame.draw.rect(screen, (255, 0, 0), bird_rect)
            for pipe in pipes:
                pygame.draw.rect(screen, (0, 128, 0), pipe)
            pygame.display.flip()
            clock.tick(60)

            if done:
                print(f"Episode: {episode+1}, Score: {score}, Best: {best_score}")
                if score > best_score:
                    best_score = score
                    torch.save(agent.q_network.state_dict(), "model.pth")
                break

        # æ¯10è½®æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
        if episode % 10 == 0:
            agent.update_target_network()

    pygame.quit()

if __name__ == "__main__":
    main()
```

---

## ğŸ“ˆ ä¸‰ã€è®­ç»ƒä¸ä¼˜åŒ–å»ºè®®

### âœ… æˆåŠŸå…³é”®ç‚¹ï¼š
- **çŠ¶æ€è®¾è®¡**ï¼š`(é¸Ÿçš„ä½ç½®, é€Ÿåº¦, åˆ°ä¸‹ä¸€ä¸ªç®¡é“çš„æ°´å¹³è·ç¦», å‚ç›´å·®è·)` æ˜¯å…³é”®ã€‚
- **å¥–åŠ±å‡½æ•°**ï¼š
    - å­˜æ´»ï¼š+0.1
    - é€šè¿‡ç®¡é“ï¼š+10
    - æ’å‡»ï¼š-10
- **Îµ-greedy æ¢ç´¢**ï¼šä» 1.0 è¡°å‡åˆ° 0.01ã€‚
- **ç›®æ ‡ç½‘ç»œæ›´æ–°**ï¼šæ¯ 10 è½®åŒæ­¥ä¸€æ¬¡ã€‚
- **ç»éªŒå›æ”¾**ï¼šå­˜å‚¨ 50k æ­¥ï¼Œéšæœºé‡‡æ ·è®­ç»ƒã€‚

### ğŸš€ è¿›é˜¶ä¼˜åŒ–ï¼š
- ä½¿ç”¨ **Double DQN** æˆ– **Dueling DQN** æå‡æ€§èƒ½ã€‚
- æ”¹ç”¨ **å›¾åƒè¾“å…¥**ï¼ˆæˆªå›¾ä½œä¸ºçŠ¶æ€ï¼‰ï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚
- ä½¿ç”¨ **PPO** ç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œæ›´é€‚åˆè¿ç»­æ§åˆ¶ã€‚
- æ·»åŠ  **ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPrioritized Replayï¼‰**ã€‚

---

## ğŸ“ å››ã€è¿è¡Œè¯´æ˜

1. å‡†å¤‡ `pygame` èµ„æºæ–‡ä»¶ï¼ˆå›¾ç‰‡ã€éŸ³æ•ˆï¼‰æˆ–ä½¿ç”¨çº¯è‰²çŸ©å½¢ä»£æ›¿ã€‚
2. è¿è¡Œ `python train.py` å¼€å§‹è®­ç»ƒã€‚
3. è®­ç»ƒæ•°å°æ—¶åï¼ŒAI å°†å­¦ä¼šç¨³å®šé£è¡Œã€‚
4. å¯åŠ è½½ `model.pth` è¿›è¡Œæµ‹è¯•ã€‚

---

## ğŸŒŸ äº”ã€æ‰©å±•é¡¹ç›®

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **å›¾åƒç‰ˆ DQN** | è¾“å…¥æ¸¸æˆæˆªå›¾ï¼ˆ84x84ç°åº¦å›¾ï¼‰ï¼Œä½¿ç”¨ CNN æå–ç‰¹å¾ |
| **Web ç‰ˆ Flappy Bird + AI** | ç”¨ HTML5 Canvas å®ç°ï¼ŒJS è°ƒç”¨æ¨¡å‹ |
| **å¤šæ™ºèƒ½ä½“ç«äº‰** | å¤šåªé¸ŸåŒæ—¶é£è¡Œï¼Œå¼•å…¥ç«äº‰æœºåˆ¶ |
| **è¿ç§»å­¦ä¹ ** | åœ¨ä¸åŒéš¾åº¦ä¸‹è¿ç§»ç­–ç•¥ |

---