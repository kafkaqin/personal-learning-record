å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ `scikit-optimize`ï¼ˆç®€ç§° `skopt`ï¼‰å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–çš„å®Œæ•´ç¤ºä¾‹ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ **éšæœºæ£®æ—åˆ†ç±»å™¨**ï¼ˆRandom Forestï¼‰åœ¨ `Iris` æ•°æ®é›†ä¸Šï¼Œé€šè¿‡ `skopt` çš„è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰æ¥å¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°ç»„åˆã€‚

---

## âœ… ç›®æ ‡

ä½¿ç”¨ `scikit-optimize` ä¼˜åŒ–ä»¥ä¸‹è¶…å‚æ•°ï¼š

- `n_estimators`: å†³ç­–æ ‘æ•°é‡ï¼ˆæ•´æ•°ï¼‰
- `max_depth`: æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆæ•´æ•°æˆ– Noneï¼‰
- `min_samples_split`: åˆ†è£‚å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼ˆæ•´æ•°ï¼‰
- `min_samples_leaf`: å¶èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼ˆæ•´æ•°ï¼‰

---

## ğŸ§° å®‰è£…ä¾èµ–

```bash
pip install scikit-optimize scikit-learn matplotlib
```

---

## ğŸ“Š ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨ `skopt` ä¼˜åŒ–éšæœºæ£®æ—è¶…å‚æ•°

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from skopt import gp_minimize
from skopt.space import Integer
from skopt.utils import use_named_args

# 1. åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

# 2. å®šä¹‰æœç´¢ç©ºé—´
dim_n_estimators = Integer(10, 200, name='n_estimators')
dim_max_depth = Integer(1, 20, name='max_depth')
dim_min_samples_split = Integer(2, 20, name='min_samples_split')
dim_min_samples_leaf = Integer(1, 20, name='min_samples_leaf')

dimensions = [dim_n_estimators, dim_max_depth, dim_min_samples_split, dim_min_samples_leaf]

# é»˜è®¤å‚æ•°ï¼ˆåˆå§‹ç‚¹ï¼‰
default_params = [100, 10, 2, 1]

# 3. å®šä¹‰ç›®æ ‡å‡½æ•°ï¼ˆè¦æœ€å°åŒ–çš„ç›®æ ‡ï¼šè´Ÿçš„äº¤å‰éªŒè¯å‡†ç¡®ç‡ï¼‰
rf = RandomForestClassifier(random_state=42)

@use_named_args(dimensions)
def objective(**params):
    rf.set_params(**params)
    return -cross_val_score(rf, X, y, cv=5, n_jobs=-1, scoring='accuracy').mean()

# 4. æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
search_result = gp_minimize(
    func=objective,
    dimensions=dimensions,
    acq_func='EI',           # é‡‡é›†å‡½æ•°ï¼šæœŸæœ›æ”¹è¿›ï¼ˆExpected Improvementï¼‰
    n_calls=50,              # è¿­ä»£æ¬¡æ•°
    x0=default_params,       # åˆå§‹å‚æ•°
    random_state=42
)

# 5. è¾“å‡ºç»“æœ
print("æœ€ä¼˜è¶…å‚æ•°ï¼š")
print(f"n_estimators = {search_result.x[0]}")
print(f"max_depth = {search_result.x[1]}")
print(f"min_samples_split = {search_result.x[2]}")
print(f"min_samples_leaf = {search_result.x[3]}")

print(f"\næœ€ä¼˜äº¤å‰éªŒè¯å‡†ç¡®ç‡: {-search_result.fun:.4f}")

# 6. ï¼ˆå¯é€‰ï¼‰ç»˜åˆ¶ä¼˜åŒ–è¿‡ç¨‹
from skopt.plots import plot_convergence
plot_convergence(search_result)
plt.show()
```

---

## ğŸ“ˆ è¾“å‡ºç¤ºä¾‹

```
æœ€ä¼˜è¶…å‚æ•°ï¼š
n_estimators = 148
max_depth = 12
min_samples_split = 5
min_samples_leaf = 2

æœ€ä¼˜äº¤å‰éªŒè¯å‡†ç¡®ç‡: 0.9667
```

`plot_convergence()` ä¼šæ˜¾ç¤ºç›®æ ‡å‡½æ•°å€¼éšè¿­ä»£æ¬¡æ•°ä¸‹é™çš„è¶‹åŠ¿ï¼Œå¸®åŠ©ä½ åˆ¤æ–­ä¼˜åŒ–æ˜¯å¦æ”¶æ•›ã€‚

---

## ğŸ” è¯´æ˜

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| `gp_minimize` | ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGaussian Processï¼‰è¿›è¡Œè´å¶æ–¯ä¼˜åŒ– |
| `Integer` | å®šä¹‰æ•´æ•°å‹è¶…å‚æ•°æœç´¢ç©ºé—´ |
| `@use_named_args` | å…è®¸ä½¿ç”¨å‘½åå‚æ•°ä¼ é€’ç»™ç›®æ ‡å‡½æ•° |
| `acq_func='EI'` | ä½¿ç”¨â€œæœŸæœ›æ”¹è¿›â€ç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹ |
| `n_calls` | æ§åˆ¶ä¼˜åŒ–è¿­ä»£æ¬¡æ•°ï¼Œè¶Šå¤šè¶Šå¯èƒ½æ‰¾åˆ°æœ€ä¼˜ï¼Œä½†è€—æ—¶æ›´é•¿ |

---

## âœ… ä¼˜åŠ¿å¯¹æ¯”ç½‘æ ¼æœç´¢/éšæœºæœç´¢

- **æ›´é«˜æ•ˆ**ï¼šè´å¶æ–¯ä¼˜åŒ–åˆ©ç”¨å†å²è¯„ä¼°ç»“æœå»ºæ¨¡ï¼Œæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå€™é€‰ç‚¹ã€‚
- **é€‚åˆæ˜‚è´µçš„è¯„ä¼°**ï¼šå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒã€å¤§è§„æ¨¡æ•°æ®è®­ç»ƒç­‰ã€‚
- **æ”¯æŒè¿ç»­ã€ç¦»æ•£ã€æ¡ä»¶ç©ºé—´**ã€‚

---

## ğŸš€ æ‰©å±•å»ºè®®

- ç»“åˆ `Pipeline` å’Œ `sklearn` æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚
- ä½¿ç”¨ `dump` å’Œ `load` ä¿å­˜/æ¢å¤ä¼˜åŒ–ç»“æœã€‚
- æ”¯æŒæ¡ä»¶è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼šä»…å½“ `criterion='tree'` æ—¶æ‰ä¼˜åŒ– `max_leaf_nodes`ï¼‰ã€‚
- æ›¿æ¢ä¸º `forest_minimize`ï¼ˆéšæœºæ£®æ—å›å½’å™¨å»ºæ¨¡ï¼‰æˆ– `gbrt_minimize`ï¼ˆæ¢¯åº¦æå‡æ ‘ï¼‰ã€‚

---