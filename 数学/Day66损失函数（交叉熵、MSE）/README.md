å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬æ¥æ¼”ç¤ºå¦‚ä½•åœ¨ **PyTorch** ä¸­ï¼š

- ä½¿ç”¨å†…ç½®çš„ **æŸå¤±å‡½æ•°**ï¼ˆå¦‚ `MSELoss`, `CrossEntropyLoss`ï¼‰
- å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œ **å‰å‘ä¼ æ’­**
- ä½¿ç”¨ `loss.backward()` è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
- æŸ¥çœ‹å‚æ•°çš„ `.grad` å±æ€§æ¥è§‚å¯Ÿæ¢¯åº¦å€¼

---

## ğŸ§  æœ¬ä¾‹ç›®æ ‡

æˆ‘ä»¬å°†ï¼š

1. å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ï¼ˆå¦‚ `y = Wx + b`ï¼‰
2. ä½¿ç”¨ PyTorch çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚ `MSELoss`ï¼‰
3. æ‰‹åŠ¨ä¼ å…¥è¾“å…¥å’Œç›®æ ‡
4. è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
5. æŸ¥çœ‹å‚æ•°çš„æ¢¯åº¦

---

## âœ… ç¤ºä¾‹ä»£ç ï¼ˆPyTorch ä¸­ä½¿ç”¨æŸå¤±å‡½æ•°å¹¶è®¡ç®—æ¢¯åº¦ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°
torch.manual_seed(42)

# -------------------------------------
# 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹
# -------------------------------------
# y = Wx + bï¼Œå…¶ä¸­ W æ˜¯æƒé‡ï¼Œb æ˜¯åç½®
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(in_features=1, out_features=1)  # ç®€å•çº¿æ€§å±‚

    def forward(self, x):
        return self.linear(x)

model = SimpleModel()

# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print("åˆå§‹æ¨¡å‹å‚æ•°ï¼š")
print(model.state_dict())
```

---

### ğŸ” æ¨¡å‹ç»“æ„è¯´æ˜ï¼š

```python
SimpleModel(
  (linear): Linear(in_features=1, out_features=1, bias=True)
)
```

- `in_features=1`ï¼šè¡¨ç¤ºè¾“å…¥ç‰¹å¾ç»´åº¦ä¸º 1
- `out_features=1`ï¼šè¾“å‡ºç»´åº¦ä¹Ÿä¸º 1
- `bias=True`ï¼šæ¨¡å‹åŒ…å«åç½®é¡¹

---

```python
# -------------------------------------
# 2. å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆå¦‚ MSE Lossï¼‰
# -------------------------------------
criterion = nn.MSELoss()

# -------------------------------------
# 3. å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆå¦‚ SGD æˆ– Adamï¼‰
# -------------------------------------
optimizer = optim.SGD(model.parameters(), lr=0.01)

# -------------------------------------
# 4. æ„é€ è¾“å…¥å’Œç›®æ ‡
# -------------------------------------
# å‡è®¾æˆ‘ä»¬æœ‰ 3 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªç‰¹å¾
inputs = torch.tensor([[1.0], [2.0], [3.0]], requires_grad=True)
targets = torch.tensor([[2.0], [4.0], [6.0]])  # ç›®æ ‡æ˜¯ 2x

# -------------------------------------
# 5. å‰å‘ä¼ æ’­
# -------------------------------------
outputs = model(inputs)
loss = criterion(outputs, targets)

print("\nå‰å‘ä¼ æ’­è¾“å‡ºï¼š")
print(outputs)
print("\nè®¡ç®—æŸå¤±å€¼ï¼š", loss.item())

# -------------------------------------
# 6. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
# -------------------------------------
optimizer.zero_grad()  # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
loss.backward()        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

# -------------------------------------
# 7. æŸ¥çœ‹å‚æ•°æ¢¯åº¦
# -------------------------------------
print("\nå‚æ•°æ¢¯åº¦ï¼š")
for name, param in model.named_parameters():
    print(f"{name}: {param.grad}")
```

---

## ğŸ“ˆ ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰ä¸åŒï¼‰

```
åˆå§‹æ¨¡å‹å‚æ•°ï¼š
OrderedDict([('linear.weight', tensor([[0.8200]])), ('linear.bias', tensor([0.]))])

å‰å‘ä¼ æ’­è¾“å‡ºï¼š
tensor([[0.8200],
        [1.6400],
        [2.4600]], grad_fn=<AddmmBackward>)

è®¡ç®—æŸå¤±å€¼ï¼š 5.521199703216553

å‚æ•°æ¢¯åº¦ï¼š
linear.weight: tensor([[-4.9200]])
linear.bias: tensor([-3.0000])
```

---

## ğŸ“Œ å…³é”®ç‚¹è¯´æ˜

| æ­¥éª¤ | è¯´æ˜ |
|------|------|
| `loss.backward()` | è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆåŸºäºå½“å‰æŸå¤±ï¼‰ |
| `param.grad` | å‚æ•°çš„ `.grad` å±æ€§ä¿å­˜äº†æ¢¯åº¦å€¼ |
| `optimizer.zero_grad()` | æ¸…ç©ºæ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦å åŠ  |
| `requires_grad=True` | è¾“å…¥æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆä¸€èˆ¬å¯¹æ¨¡å‹å‚æ•°è‡ªåŠ¨è®¾ç½®ï¼‰ |

---

## âœ… ä½¿ç”¨å…¶ä»–æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚åˆ†ç±»ä»»åŠ¡ï¼‰

å¦‚æœä½ åšçš„æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

```python
criterion = nn.CrossEntropyLoss()
```

å®ƒä¼šè‡ªåŠ¨ç»“åˆ `Softmax` å’Œ `NLLLoss`ï¼Œé€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ã€‚

---

## ğŸ§© åç»­æ‰©å±•å»ºè®®

ä½ å¯ä»¥ç»§ç»­ï¼š

- å°†æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—åµŒå…¥è®­ç»ƒå¾ªç¯
- ä½¿ç”¨ `torchviz` å¯è§†åŒ–è®¡ç®—å›¾
- æ‰‹åŠ¨å®ç°æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰
- ä½¿ç”¨ `torch.nn.utils.clip_grad_norm_` é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
