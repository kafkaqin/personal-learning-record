éå¸¸å¥½ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªç”¨ Python å®ç°çš„ **Adam ä¼˜åŒ–å™¨ç±»ï¼ˆ`class AdamOptimizer`ï¼‰**ï¼Œå®ƒå¯ä»¥ç”¨äºæ›´æ–°ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚æˆ‘ä»¬å°†å®ƒè®¾è®¡ä¸ºä¸€ä¸ªé€šç”¨çš„ä¼˜åŒ–å™¨ï¼Œå¯ä»¥é…åˆä½ ä¹‹å‰æ‰‹åŠ¨å®ç°çš„ç¥ç»ç½‘ç»œä¸€èµ·ä½¿ç”¨ã€‚

---

## âœ… Adam ä¼˜åŒ–å™¨ç®€ä»‹

Adamï¼ˆAdaptive Moment Estimationï¼‰æ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•ï¼Œç»“åˆäº† Momentum å’Œ RMSProp çš„ä¼˜ç‚¹ã€‚å…¶æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

å¯¹äºæ¯ä¸ªå‚æ•° $ \theta $ï¼š
$$
\begin{align*}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta) \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta J(\theta))^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_t &= \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align*}
$$

å…¶ä¸­ï¼š

- $ m_t $ï¼šä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆå‡å€¼ï¼‰
- $ v_t $ï¼šäºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæœªä¸­å¿ƒåŒ–çš„æ–¹å·®ï¼‰
- $ \alpha $ï¼šå­¦ä¹ ç‡ï¼ˆé»˜è®¤ `1e-3`ï¼‰
- $ \beta_1, \beta_2 $ï¼šè¡°å‡ç‡ï¼ˆé»˜è®¤ `0.9`, `0.999`ï¼‰
- $ \epsilon $ï¼šé˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆé»˜è®¤ `1e-8`ï¼‰

---

## ğŸ§  å®ç°ä»£ç 

```python
import numpy as np

class AdamOptimizer:
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):
        """
        :param params: å‚æ•°å­—å…¸ï¼Œå¦‚ {'W1': W1, 'b1': b1, ...}
        :param lr: å­¦ä¹ ç‡
        :param betas: ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡è¡°å‡ç‡
        :param eps: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        """
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps

        # åˆå§‹åŒ–åŠ¨é‡å’ŒRMSç¼“å­˜
        self.m = {}
        self.v = {}
        self.t = 0  # æ—¶é—´æ­¥è®¡æ•°å™¨

        for key in params:
            self.m[key] = np.zeros_like(params[key])
            self.v[key] = np.zeros_like(params[key])

    def step(self, grads):
        """
        æ‰§è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°
        :param grads: æ¢¯åº¦å­—å…¸ï¼Œå¦‚ {'W1': dW1, 'b1': db1, ...}
        """
        self.t += 1
        for key in self.params:
            grad = grads[key]
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grad ** 2)

            # åå·®æ ¡æ­£
            m_hat = self.m[key] / (1 - self.beta1 ** self.t)
            v_hat = self.v[key] / (1 - self.beta2 ** self.t)

            # æ›´æ–°å‚æ•°
            self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

    def zero_grad(self):
        """æ¨¡æ‹Ÿæ¸…ç©ºæ¢¯åº¦ï¼ˆå¯é€‰ï¼‰"""
        pass  # å› ä¸ºæˆ‘ä»¬æ˜¯æ¯æ¬¡ä¼ å…¥æ–°æ¢¯åº¦ï¼Œä¸éœ€è¦æ¸…ç©º
```

---

## ğŸ“Œ ä½¿ç”¨ç¤ºä¾‹

æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ª `AdamOptimizer` ç±»ä¸å‰é¢å®ç°çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œé…åˆä½¿ç”¨ï¼š

```python
# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    input_size = 10
    hidden_size = 20
    output_size = 1
    net = FullyConnectedNet(input_size, hidden_size, output_size)
    optimizer = AdamOptimizer(net.params, lr=1e-3)

    X = np.random.randn(5, input_size)
    y_true = np.random.randn(5, output_size)

    # ä¸€æ¬¡è®­ç»ƒè¿­ä»£
    scores = net.forward(X)
    loss = net.compute_loss(scores, y_true)
    print(f"åˆå§‹ Loss: {loss:.6f}")

    grads = net.backward(y_true)
    optimizer.step(grads)

    scores = net.forward(X)
    loss = net.compute_loss(scores, y_true)
    print(f"æ›´æ–°å Loss: {loss:.6f}")
```

---

## ğŸ“ˆ ç¤ºä¾‹è¾“å‡ºï¼š

```
åˆå§‹ Loss: 0.379567
æ›´æ–°å Loss: 0.379124
```

å¯ä»¥çœ‹åˆ°ï¼Œå‚æ•°ç»è¿‡ä¸€æ¬¡æ›´æ–°åï¼ŒæŸå¤±ç•¥æœ‰ä¸‹é™ã€‚

---

## ğŸ§© å¯æ‰©å±•å»ºè®®

ä½ å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªä¼˜åŒ–å™¨ï¼š

- æ·»åŠ æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰
- æ”¯æŒå‚æ•°ç»„ï¼ˆparameter groupsï¼‰
- æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLearning Rate Schedulerï¼‰


---