éå¸¸å¥½çš„ä»»åŠ¡ï¼æˆ‘ä»¬å°†ä½¿ç”¨ **PyTorch** å®ç°ä¸€ä¸ª **ç®€å•çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰**ï¼Œç”¨äºç”Ÿæˆ **MNIST æ‰‹å†™æ•°å­—å›¾åƒ**ã€‚

---

## ğŸ§  GAN ç®€ä»‹

GAN ç”±ä¸¤ä¸ªç½‘ç»œç»„æˆï¼š

| ç½‘ç»œ | åŠŸèƒ½ |
|------|------|
| **ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰** | ä»éšæœºå™ªå£°ç”Ÿæˆå›¾åƒ |
| **åˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰** | åˆ¤æ–­å›¾åƒæ˜¯çœŸå®è¿˜æ˜¯ç”Ÿæˆçš„ |

è®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€ä¸ª **é›¶å’Œåšå¼ˆï¼ˆzero-sum gameï¼‰**ï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼š

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

---

## âœ… æœ¬ä¾‹ç›®æ ‡

æˆ‘ä»¬å°†å®ç°ï¼š

- ä¸€ä¸ªç®€å•çš„ **å…¨è¿æ¥ GANï¼ˆDCGAN çš„ç®€åŒ–ç‰ˆï¼‰**
- ä½¿ç”¨ **MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†**
- ä½¿ç”¨ **Binary Cross Entropy Lossï¼ˆBCELossï¼‰**
- ä½¿ç”¨ **Adam ä¼˜åŒ–å™¨**
- æ¯éš”å‡ ä¸ª epoch å¯è§†åŒ–ç”Ÿæˆçš„å›¾åƒ

---

## ğŸ“¦ æ‰€éœ€åº“

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
```

---

## ğŸ§± å®šä¹‰ Generatorï¼ˆç”Ÿæˆå™¨ï¼‰

```python
class Generator(nn.Module):
    def __init__(self, input_dim=100, output_dim=784):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
        )

    def forward(self, z):
        return self.net(z)
```

---

## ğŸ§± å®šä¹‰ Discriminatorï¼ˆåˆ¤åˆ«å™¨ï¼‰

```python
class Discriminator(nn.Module):
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()  # è¾“å‡ºæ¦‚ç‡
        )

    def forward(self, x):
        return self.net(x)
```

---

## ğŸ“¥ åŠ è½½ MNIST æ•°æ®é›†

```python
# æ•°æ®é¢„å¤„ç†ï¼šå½’ä¸€åŒ–åˆ° [-1, 1]ï¼Œå› ä¸º Generator è¾“å‡ºæ˜¯ Tanh
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)
```

---

## ğŸš€ åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å‚æ•°è®¾ç½®
latent_dim = 100
lr = 0.0002
epochs = 100

# åˆå§‹åŒ–æ¨¡å‹
generator = Generator(latent_dim).to(device)
discriminator = Discriminator().to(device)

# æŸå¤±å‡½æ•°
criterion = nn.BCELoss()

# ä¼˜åŒ–å™¨
optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
```

---

## ğŸ“ˆ å¯è§†åŒ–ç”Ÿæˆå›¾åƒå‡½æ•°

```python
def visualize_images(generator, device, epoch):
    z = torch.randn(16, latent_dim).to(device)
    with torch.no_grad():
        generated = generator(z).cpu().view(-1, 28, 28)

    plt.figure(figsize=(4, 4))
    for i in range(16):
        plt.subplot(4, 4, i+1)
        plt.imshow(generated[i], cmap='gray')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig(f"generated_epoch_{epoch}.png")
    plt.close()
```

---

## ğŸ è®­ç»ƒå¾ªç¯

```python
for epoch in range(epochs):
    for i, (real_images, _) in enumerate(train_loader):
        batch_size = real_images.size(0)
        real_images = real_images.view(batch_size, -1).to(device)
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # ---------------------
        #  è®­ç»ƒåˆ¤åˆ«å™¨ D
        # ---------------------
        # çœŸå®å›¾åƒçš„æŸå¤±
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)

        # ç”Ÿæˆå™¨ç”Ÿæˆå‡å›¾åƒ
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = generator(z)
        fake_outputs = discriminator(fake_images)
        d_loss_fake = criterion(fake_outputs, fake_labels)

        d_loss = d_loss_real + d_loss_fake

        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # ---------------------
        #  è®­ç»ƒç”Ÿæˆå™¨ G
        # ---------------------
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_images = generator(z)
        outputs = discriminator(fake_images)
        g_loss = criterion(outputs, real_labels)

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    print(f"Epoch [{epoch+1}/{epochs}] Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}")

    # æ¯ 10 ä¸ª epoch å¯è§†åŒ–ä¸€æ¬¡
    if (epoch + 1) % 10 == 0:
        visualize_images(generator, device, epoch + 1)
```

---

## ğŸ“ˆ ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰ä¸åŒï¼‰

```
Epoch [1/100] Loss D: 1.3689, Loss G: 0.7231
Epoch [2/100] Loss D: 1.1452, Loss G: 0.6213
...
Epoch [100/100] Loss D: 0.3214, Loss G: 0.6901
```

å¯è§†åŒ–å›¾åƒä¼šä¿å­˜ä¸º `generated_epoch_10.png`ã€`generated_epoch_20.png` ç­‰ã€‚

---

## âœ… æ€»ç»“å¯¹æ¯”è¡¨

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| Generator | ä»éšæœºå™ªå£°ç”Ÿæˆå›¾åƒï¼ˆ784ç»´ï¼‰ |
| Discriminator | åˆ¤æ–­å›¾åƒæ˜¯å¦ä¸ºçœŸ |
| æŸå¤±å‡½æ•° | BCE Lossï¼ˆBinary Cross Entropyï¼‰ |
| ä¼˜åŒ–å™¨ | Adamï¼ˆå­¦ä¹ ç‡ 0.0002ï¼‰ |
| æ•ˆæœ | 50 ä¸ª epoch åå¯ç”Ÿæˆè¾ƒæ¸…æ™°çš„æ•°å­—å›¾åƒ |

---

## ğŸ§© è¿›ä¸€æ­¥æ‰©å±•å»ºè®®

ä½ å¯ä»¥ç»§ç»­ï¼š

- ä½¿ç”¨ **DCGANï¼ˆæ·±åº¦å·ç§¯ GANï¼‰** æå‡å›¾åƒè´¨é‡
- ä½¿ç”¨ **Wasserstein GANï¼ˆWGANï¼‰** æ”¹è¿›è®­ç»ƒç¨³å®šæ€§
- ä½¿ç”¨ **LSGANï¼ˆæœ€å°äºŒä¹˜ GANï¼‰** æ›¿ä»£ BCE Loss
- æ·»åŠ  **æ³¨æ„åŠ›æœºåˆ¶** åˆ°ç”Ÿæˆå™¨æˆ–åˆ¤åˆ«å™¨
- å°† GAN åº”ç”¨äº **å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆå¦‚ Pix2Pixï¼‰**

---