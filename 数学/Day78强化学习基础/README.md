ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ **OpenAI Gym** å’Œ **Q-learning** ç®—æ³•è§£å†³ç®€å•è¿·å®«é—®é¢˜çš„å®Œæ•´ç¤ºä¾‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Gym ä¸­çš„ `FrozenLake-v1` ç¯å¢ƒï¼Œå®ƒæ˜¯ä¸€ä¸ªç»å…¸çš„è¿·å®«ï¼ˆgrid worldï¼‰é—®é¢˜ã€‚

---

## ğŸ§© é—®é¢˜æè¿°ï¼šFrozenLake-v1

- ç¯å¢ƒæ˜¯ä¸€ä¸ª 4x4 çš„ç½‘æ ¼è¿·å®«ã€‚
- ç©å®¶ä»èµ·ç‚¹ Sï¼ˆ(0,0)ï¼‰å‡ºå‘ï¼Œç›®æ ‡æ˜¯å®‰å…¨åœ°èµ°åˆ°ç»ˆç‚¹ Gï¼ˆ(3,3)ï¼‰ã€‚
- æ¯ä¸ªæ ¼å­å¯èƒ½æ˜¯ï¼š
    - `S`: Startï¼ˆèµ·ç‚¹ï¼‰
    - `F`: Frozenï¼ˆå®‰å…¨ï¼‰
    - `H`: Holeï¼ˆæ‰ä¸‹å»å°±å¤±è´¥ï¼‰
    - `G`: Goalï¼ˆç»ˆç‚¹ï¼‰
- åŠ¨ä½œç©ºé—´ï¼š`0=å·¦`, `1=ä¸‹`, `2=å³`, `3=ä¸Š`
- çŠ¶æ€ç©ºé—´ï¼šå…± 16 ä¸ªçŠ¶æ€ï¼ˆ0~15ï¼‰

---

## ğŸ§  Q-learning ç®€ä»‹

Q-learning æ˜¯ä¸€ç§æ— æ¨¡å‹ï¼ˆmodel-freeï¼‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºå­¦ä¹ ä¸€ä¸ª Q è¡¨ï¼ˆQ-tableï¼‰ï¼š

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'}Q(s', a') - Q(s, a) \right]
$$

å…¶ä¸­ï¼š
- $ s $: å½“å‰çŠ¶æ€
- $ a $: å½“å‰åŠ¨ä½œ
- $ r $: å¥–åŠ±
- $ s' $: ä¸‹ä¸€çŠ¶æ€
- $ \alpha $: å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰
- $ \gamma $: æŠ˜æ‰£å› å­ï¼ˆdiscount factorï¼‰

---

## ğŸ§ª æ­¥éª¤æ¦‚è§ˆ

1. åˆ›å»ºç¯å¢ƒ
2. åˆå§‹åŒ– Q-table
3. ä½¿ç”¨ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
4. æ›´æ–° Q-table
5. è®­ç»ƒæ¨¡å‹
6. æµ‹è¯•ç­–ç•¥

---

## ğŸ§° å®‰è£… OpenAI Gym

```bash
pip install gym
```

---

## ğŸ“Š ç¤ºä¾‹ä»£ç ï¼šQ-learning è§£å†³ FrozenLake

```python
import gym
import numpy as np
import random
from gym import wrappers

# åˆ›å»ºç¯å¢ƒ
env = gym.make('FrozenLake-v1', is_slippery=True)  # å¯ä»¥è®¾ç½® is_slippery=False æ¥ç®€åŒ–é—®é¢˜

# åˆå§‹åŒ– Q-tableï¼ˆ16 ä¸ªçŠ¶æ€ï¼Œ4 ä¸ªåŠ¨ä½œï¼‰
q_table = np.zeros([env.observation_space.n, env.action_space.n])

# è¶…å‚æ•°
learning_rate = 0.8
discount_factor = 0.95
epsilon = 1.0
min_epsilon = 0.01
epsilon_decay = 0.995
episodes = 5000

# Q-learning è®­ç»ƒ
for i in range(episodes):
    state = env.reset()[0]  # è¿”å›çš„æ˜¯ä¸€ä¸ª tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
    done = False

    while not done:
        # Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # éšæœºåŠ¨ä½œ
        else:
            action = np.argmax(q_table[state])

        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, _, info = env.step(action)

        # æ›´æ–° Q-table
        q_table[state, action] = q_table[state, action] + learning_rate * (
                reward + discount_factor * np.max(q_table[next_state]) - q_table[state, action]
        )

        state = next_state

    # è¡°å‡ epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

print("Q-table å·²è®­ç»ƒå®Œæˆï¼")
```

---

## ğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„ Q-learning ç­–ç•¥

```python
# æµ‹è¯•ç­–ç•¥
state = env.reset()[0]
done = False
steps = 0

print("æµ‹è¯•è·¯å¾„ï¼š")
while not done and steps < 100:
    action = np.argmax(q_table[state])
    print(f"çŠ¶æ€ {state} â†’ åŠ¨ä½œ {action}")
    next_state, reward, done, _, _ = env.step(action)
    state = next_state
    steps += 1

if reward == 1:
    print("ğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼")
else:
    print("ğŸ’€ æ‰å…¥é™·é˜±æˆ–è¶…æ—¶ã€‚")
```

---

## ğŸ“ˆ å¯è§†åŒ– Q-tableï¼ˆå¯é€‰ï¼‰

```python
import seaborn as sns
import matplotlib.pyplot as plt

# ç»˜åˆ¶ Q-table
plt.figure(figsize=(10, 6))
sns.heatmap(q_table, annot=True, cmap="YlGnBu", cbar=True)
plt.title("Q-table (çŠ¶æ€ x åŠ¨ä½œ)")
plt.xlabel("åŠ¨ä½œ")
plt.ylabel("çŠ¶æ€")
plt.show()
```

---

## ğŸ“‹ è¾“å‡ºç¤ºä¾‹

```
Q-table å·²è®­ç»ƒå®Œæˆï¼

æµ‹è¯•è·¯å¾„ï¼š
çŠ¶æ€ 0 â†’ åŠ¨ä½œ 1
çŠ¶æ€ 4 â†’ åŠ¨ä½œ 1
çŠ¶æ€ 8 â†’ åŠ¨ä½œ 2
çŠ¶æ€ 9 â†’ åŠ¨ä½œ 1
çŠ¶æ€ 13 â†’ åŠ¨ä½œ 2
çŠ¶æ€ 14 â†’ åŠ¨ä½œ 2
ğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼
```

---

## ğŸ§  å°è´´å£«

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| `is_slippery=True` | ç¯å¢ƒå…·æœ‰ä¸ç¡®å®šæ€§ï¼Œå¢åŠ éš¾åº¦ |
| `epsilon_decay` | æ§åˆ¶æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ |
| `learning_rate` | å­¦ä¹ æ­¥é•¿ï¼Œä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå° |
| `discount_factor` | æœªæ¥å¥–åŠ±çš„æŠ˜æ‰£ï¼Œè¶Šæ¥è¿‘ 1 è¶Šé‡è§†é•¿æœŸæ”¶ç›Š |

---

## âœ… æ‰©å±•å»ºè®®

- ä½¿ç”¨ç¥ç»ç½‘ç»œæ›¿ä»£ Q-tableï¼ˆå³ DQNï¼‰
- ä½¿ç”¨ `gym.wrappers.Monitor` å½•åˆ¶è®­ç»ƒè¿‡ç¨‹
- æ›´å¤æ‚çš„è¿·å®«ï¼ˆå¦‚ 8x8ï¼‰
- æ·»åŠ å¯è§†åŒ–ç•Œé¢ï¼ˆå¦‚ PyGameï¼‰

---