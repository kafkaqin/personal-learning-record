{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3729fbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1905, -0.1380,  0.0555,  0.0723,  0.1181,  0.1635,  0.3352, -0.2762,\n",
       "         -0.0397, -0.1579],\n",
       "        [-0.1147, -0.0477,  0.2063,  0.0972,  0.2191,  0.0822,  0.1356, -0.1861,\n",
       "          0.0031, -0.1109]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ab57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0101, -0.0470,  0.1398,  0.1965,  0.0596, -0.0679, -0.0860,  0.2774,\n",
       "         -0.0793,  0.2662],\n",
       "        [ 0.0097,  0.0502,  0.1373,  0.0079,  0.1435, -0.0539,  0.0214,  0.1611,\n",
       "         -0.1975,  0.0745]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae96166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2323,  0.3271,  0.1461,  0.3713,  0.0716,  0.0178,  0.1293, -0.2017,\n",
       "          0.2081,  0.1452],\n",
       "        [-0.2523,  0.2383,  0.0846,  0.2369,  0.2055, -0.0072,  0.2755, -0.3014,\n",
       "          0.2950,  0.0110]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d83c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0577, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMPL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=True)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "net = FixedHiddenMPL()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0dab6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0483, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                                nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMPL())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db7640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
